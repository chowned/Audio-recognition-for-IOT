{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import of necessary parts for google drive"
      ],
      "metadata": {
        "id": "LIybt8lv4sju"
      },
      "id": "LIybt8lv4sju"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "t1BIlB1O6eQ8",
        "outputId": "401fa2ce-fdea-49ca-943e-11c58ad3590e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "t1BIlB1O6eQ8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "metadata": {
        "id": "Acu8TupveCFE"
      },
      "id": "Acu8TupveCFE",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/datasets/dsl_data/\")"
      ],
      "metadata": {
        "id": "to-3QVHj7uFP"
      },
      "id": "to-3QVHj7uFP",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "EcdlHyi67tOu",
        "outputId": "6aa66744-241f-4495-dc1e-e58a15f53cc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EcdlHyi67tOu",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t __MACOSX\t\t saved_models\n",
            "checkpoints\t models\t\t\t tensorboard_data\n",
            "development.csv  preprocessingGoogle.py  Test_Dataset_Truncated\n",
            "dsl_data.zip\t __pycache__\t\t Train_Dataset_Truncated\n",
            "evaluation.csv\t sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive"
      ],
      "metadata": {
        "id": "KRAfj_Gl62dK",
        "outputId": "855c1572-ef9a-462a-cc77-14579ebbb872",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KRAfj_Gl62dK",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'drive/MyDrive': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary install of dep and import libraries"
      ],
      "metadata": {
        "id": "5shtlITI3HCT"
      },
      "id": "5shtlITI3HCT"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install git > /dev/null\n",
        "#!rm ./requiremen*\n",
        "#!rm ./preprocessing*\n",
        "#!ls\n",
        "!pip install -r ipython psutil==5.9.2 sounddevice==0.4.5 scipy==1.9.1 redis==4.3.4 tensorflow==2.10.0 tensorflow-io==0.27.0 cherrypy==18.8.0 paho-mqtt==1.6.1 > /dev/null\n",
        "!pip install -r librosa tensorflow_model_optimization pandas keras tensorflow_io > /dev/null\n",
        "!pip install tensorflow[io] > /dev/null\n",
        "!pip install tensorflow_model_optimization > /dev/null\n",
        "!pip install pydub\n"
      ],
      "metadata": {
        "id": "wKL8ZIyG6rDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f57acc-2046-45ad-9fcc-0ecd40bedf17"
      },
      "id": "wKL8ZIyG6rDt",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ipython'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'librosa'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: tensorflow 2.9.2 does not provide the extra 'io'\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "95a9c863",
      "metadata": {
        "id": "95a9c863"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tensorflow as tf\n",
        "import random\n",
        "#import tensorflow_io as tfio\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import argparse as ap\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/datasets/dsl_data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8596efa2",
      "metadata": {
        "id": "8596efa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f12ffb77-49c8-4543-a174-615161fb2965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Be sure to have tensorboard, this code will be commented in final release\n"
          ]
        }
      ],
      "source": [
        "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
        "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip of archive"
      ],
      "metadata": {
        "id": "d6-1LOFkKaFd"
      },
      "id": "d6-1LOFkKaFd"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!unzip -nq ./dsl_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNRZRYvbKcya",
        "outputId": "89eeab89-a7e6-4c03-fce5-540af2960fc9"
      },
      "id": "qNRZRYvbKcya",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t __MACOSX\t\t saved_models\n",
            "checkpoints\t models\t\t\t tensorboard_data\n",
            "development.csv  preprocessingGoogle.py  Test_Dataset_Truncated\n",
            "dsl_data.zip\t __pycache__\t\t Train_Dataset_Truncated\n",
            "evaluation.csv\t sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzxQBYDlLl6f",
        "outputId": "62a4713b-6256-4498-fde7-d6b125aed93e"
      },
      "id": "lzxQBYDlLl6f",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t __MACOSX\t\t saved_models\n",
            "checkpoints\t models\t\t\t tensorboard_data\n",
            "development.csv  preprocessingGoogle.py  Test_Dataset_Truncated\n",
            "dsl_data.zip\t __pycache__\t\t Train_Dataset_Truncated\n",
            "evaluation.csv\t sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for finding optimal length of audio files"
      ],
      "metadata": {
        "id": "nNVJJQ8Qh42Q"
      },
      "id": "nNVJJQ8Qh42Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "\n",
        "folder_path = '../../datasets/dsl_data/audio'\n",
        "\n",
        "def scan_folder(folder):\n",
        "  duration_count = {}\n",
        "  for root, dirs, files in os.walk(folder):\n",
        "    for file in files:\n",
        "      if file.endswith(\".wav\"):\n",
        "        file_path = os.path.join(root, file)\n",
        "        audio = AudioSegment.from_wav(file_path)\n",
        "        duration = len(audio)\n",
        "        if duration in duration_count:\n",
        "          duration_count[duration] += 1\n",
        "        else:\n",
        "            duration_count[duration] = 1\n",
        "  return duration_count\n",
        "\n",
        "def create_dataframe(duration_count):\n",
        "  data = {\"Duration of audio file\": list(duration_count.keys()), \n",
        "            \"Number of audio files with that duration\": list(duration_count.values())}\n",
        "  df = pd.DataFrame(data)\n",
        "  df = df.sort_values(by='Number of audio files with that duration', ascending=False)\n",
        "  return df\n",
        "\n",
        "\n",
        "# find the percentage. The duration returned in second is the size that include 1-percentage inside\n",
        "\n",
        "def find_duration(folder_path, percentage_files=0.9):\n",
        "  duration_count = {}\n",
        "  for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "      if file.endswith(\".wav\"):\n",
        "        file_path = os.path.join(root, file)\n",
        "        #print(file_path)\n",
        "        audio = AudioSegment.from_wav(file_path)\n",
        "        duration = len(audio) / 1000 #convert from ms to sec\n",
        "        if duration in duration_count:\n",
        "          duration_count[duration] += 1\n",
        "        else:\n",
        "          duration_count[duration] = 1\n",
        "    total_files = sum(duration_count.values())\n",
        "    target_files = total_files * percentage_files\n",
        "    current_count = 0\n",
        "    for duration, count in sorted(duration_count.items()):\n",
        "      current_count += count\n",
        "      if current_count >= target_files:\n",
        "        duration = round(duration)\n",
        "        print(f\"Duration of audio that makes {percentage_files*100}% of the files have that duration is: {duration} seconds\")\n",
        "        return duration\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "duration = find_duration(folder_path)\n",
        "\n",
        "pathPreprocessing = \"../../datasets/dsl_data/preprocessingGoogle.py\"\n",
        "\n",
        "with open(pathPreprocessing, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Update the 4th line\n",
        "lines[8] = \"length_calculated = {}\\n\".format(duration)\n",
        "\n",
        "\n",
        "with open(pathPreprocessing, \"w\") as file:\n",
        "    file.writelines(lines)\n",
        "#print(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZsUS3bvh9X9",
        "outputId": "6be19214-3840-40ba-a5aa-aa0a435c5b01"
      },
      "id": "wZsUS3bvh9X9",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration of audio that makes 90.0% of the files have that duration is: 4 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ../../datasets/dsl_data/preprocessingGoogle.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVX8oNgfxdji",
        "outputId": "ab45ec83-033e-4a9d-d098-b0ca706e2bc4"
      },
      "id": "ZVX8oNgfxdji",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import tensorflow as tf\n",
            "# import tensorflow_io as tfio\n",
            "import numpy as np\n",
            "LABELS = ['change languagenone', 'activatemusic', 'deactivatelights', 'increasevolume', 'decreasevolume', 'increaseheat', 'decreaseheat']\n",
            "# This is the file genarated that has the Labels that i must use for training\n",
            " \n",
            "frame_length_in_s = 0.032\n",
            "frame_step_in_s  = frame_length_in_s\n",
            "length_calculated = 4\n",
            "\n",
            "PREPROCESSING_ARGS = {\n",
            "    'downsampling_rate': 16000,\n",
            "    'frame_length_in_s': frame_length_in_s,\n",
            "    'frame_step_in_s': frame_step_in_s,\n",
            "}\n",
            "\n",
            "# TRAINING_ARGS = {\n",
            "#     'batch_size': 32,\n",
            "#     'initial_learning_rate': 0.03,\n",
            "#     'end_learning_rate': 0.001, #1.e-9,\n",
            "#     'epochs': 1\n",
            "# }\n",
            "\n",
            "\n",
            "final_sparsity = 0.01\n",
            "\n",
            "num_mel_bins = (int) ((16000 - 16000 * PREPROCESSING_ARGS['frame_length_in_s'])/(16000*PREPROCESSING_ARGS['frame_step_in_s']))+1\n",
            "print(num_mel_bins)\n",
            "\n",
            "PREPROCESSING_ARGS = {\n",
            "    **PREPROCESSING_ARGS,\n",
            "    'num_mel_bins': num_mel_bins,\n",
            "    'lower_frequency': 80,\n",
            "    'upper_frequency': 8000,\n",
            "}\n",
            "\n",
            "downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n",
            "sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n",
            "frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n",
            "print(\"Frame_length: {}\".format(frame_length))\n",
            "frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n",
            "print(\"Frame_length: {}\".format(frame_step))\n",
            "num_spectrogram_bins = frame_length // 2 + 1\n",
            "num_mel_bins = PREPROCESSING_ARGS['num_mel_bins']\n",
            "lower_frequency = PREPROCESSING_ARGS['lower_frequency']\n",
            "upper_frequency = PREPROCESSING_ARGS['upper_frequency']\n",
            "\n",
            "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
            "    num_mel_bins=num_mel_bins,\n",
            "    num_spectrogram_bins=num_spectrogram_bins,\n",
            "    sample_rate=downsampling_rate,\n",
            "    lower_edge_hertz=lower_frequency,\n",
            "    upper_edge_hertz=upper_frequency\n",
            ")\n",
            "\n",
            "def preprocess(filename):\n",
            "    audio_binary = tf.io.read_file(filename)\n",
            "\n",
            "    path_parts = tf.strings.split(filename, '_')\n",
            "    path_end = path_parts[-1]\n",
            "    file_parts = tf.strings.split(path_end, '.')\n",
            "    true_label = file_parts[0]\n",
            "    label_id = tf.argmax(true_label == LABELS)\n",
            "    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n",
            "    audio = tf.squeeze(audio, axis=-1) #all our audio are mono, drop extra axis\n",
            "    audio_padded = audio\n",
            "    stft = tf.signal.stft(\n",
            "        audio,\n",
            "        frame_length=frame_length,\n",
            "        frame_step=frame_step,\n",
            "        fft_length=frame_length\n",
            "    )\n",
            "    spectrogram = tf.abs(stft)\n",
            "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
            "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
            "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)  # channel axis\n",
            "    mfcss = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
            "\n",
            "    return mfcss, label_id\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b824503",
      "metadata": {
        "id": "8b824503"
      },
      "source": [
        " # Preprocessing for Train dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "4da2cb4f",
      "metadata": {
        "id": "4da2cb4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b323ba6e-8238-45b3-cc62-a995572b4a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31\n",
            "Frame_length: 512\n",
            "Frame_length: 512\n"
          ]
        }
      ],
      "source": [
        "import preprocessingGoogle as pr\n",
        "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
        "def process_file(file_path, flag):\n",
        "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
        "    if file_path_exists:\n",
        "        new_sr=16000\n",
        "        # identifier care\n",
        "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
        "        identifier = str(int(identifier))\n",
        "        # label constructor\n",
        "        label = \"\"\n",
        "        if flag == 1:\n",
        "            label  += \"_\"\n",
        "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
        "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
        "            label  += action + object\n",
        "        # If no label available, code will just go on\n",
        "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
        "        y, sr = librosa.load('../../datasets/'+file_path)\n",
        "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
        "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
        "        y_truncated = y_truncated[:int(pr.length_calculated*new_sr)] #if longer\n",
        "        target_length = pr.length_calculated * new_sr\n",
        "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
        "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "acdacf47",
      "metadata": {
        "scrolled": true,
        "id": "acdacf47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c5b5b4-9a64-4ed9-9d48-c4023fd5ef54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution ended\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
        "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
        "\n",
        "folder_path = '../../datasets/dsl_data/'\n",
        "\n",
        "if not os.path.isdir(new_folder_path):\n",
        "  os.makedirs(new_folder_path) # hoping to have write permissions set\n",
        "if not os.listdir(new_folder_path):\n",
        "  with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "      dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
        "      dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCK MICROSOFT AND FUCK THE FUCKING IDEA OF \n",
        "      dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCKING USING \\ FOR PATH!!!!!!!!!!111oneone!!1!!!\n",
        "      dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "      for filename in filenames:\n",
        "        file_path = os.path.join(dirpath, filename)\n",
        "        file_path = file_path.replace(\"\\\\\", \"/\")\n",
        "        executor.submit(process_file, file_path, 1)\n",
        "# print(df)\n",
        "print(\"Execution ended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2ef26e",
      "metadata": {
        "id": "8f2ef26e"
      },
      "source": [
        " # Preprocessing for Evaluation dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "66b4ba13",
      "metadata": {
        "id": "66b4ba13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92819311-cf53-4021-e7c2-6454933cee49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution ended\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('../../datasets/dsl_data/evaluation.csv', sep=',')\n",
        "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
        "folder_path = '../../datasets/dsl_data/'\n",
        "\n",
        "if not os.path.isdir(new_folder_path):\n",
        "    os.makedirs(new_folder_path)\n",
        "\n",
        "if not os.listdir(new_folder_path):\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
        "                executor.submit(process_file, file_path, 0)\n",
        "\n",
        "print(\"Execution ended\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c41b90",
      "metadata": {
        "id": "69c41b90"
      },
      "source": [
        "# Auto - updating labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c0749be3",
      "metadata": {
        "id": "c0749be3"
      },
      "outputs": [],
      "source": [
        "pathPreprocessing = \"../../datasets/dsl_data/preprocessingGoogle.py\"\n",
        "\n",
        "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
        "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
        "distinct_values = df['labels'].unique()\n",
        "\n",
        "result = 'LABELS = ['\n",
        "for value in distinct_values:\n",
        "    result += \"'\" + str(value) + \"', \"\n",
        "\n",
        "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
        "\n",
        "with open(pathPreprocessing, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Update the 4th line\n",
        "lines[3] = result\n",
        "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
        "\n",
        "with open(pathPreprocessing, \"w\") as file:\n",
        "    file.writelines(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c5a54e",
      "metadata": {
        "id": "18c5a54e"
      },
      "source": [
        "# Model creation and fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a5698194",
      "metadata": {
        "id": "a5698194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e050c9d0-6eb3-4205-f224-b016338fa588"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, help='Choosing eval_percentage', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "parser = ap.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--batch_size', default=32, type=int, help=\"Choosing batch size default is 32\")\n",
        "parser.add_argument('--initial_learning_rate', default=0.03, type=float, help=\"Choosing initial_learning_rate\")\n",
        "parser.add_argument('--end_learning_rate', default=0.001, type=float, help=\"Choosing end_learning_rate\")\n",
        "parser.add_argument('--epochs', default=50, type=int, help=\"Choosing epochs\")\n",
        "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
        "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
        "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
        "parser.add_argument('--alpha', default=1, type=float, help=\"Choosing alpha\")\n",
        "\n",
        "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")\n",
        "#,'--eval_percentage','0.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5355da28",
      "metadata": {
        "id": "5355da28"
      },
      "source": [
        "Parser arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0cbb23f1",
      "metadata": {
        "id": "0cbb23f1"
      },
      "outputs": [],
      "source": [
        "args = parser.parse_args(['--epochs','100','--batch_size','128','--pruning_initial_step','0.9','--initial_learning_rate','0.03','--end_learning_rate','0.001','--alpha','0.125'])\n",
        "# args = parser.parse_args()\n",
        "num_units = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169533d1",
      "metadata": {
        "id": "169533d1"
      },
      "source": [
        "## This part of the code exist to manage all the folders\n",
        "## Please be careful, if the directories tree is not respected, the code will not work properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3c6a22f4",
      "metadata": {
        "id": "3c6a22f4"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "795af38c",
      "metadata": {
        "id": "795af38c"
      },
      "outputs": [],
      "source": [
        "# Useful to save tensorboard data\n",
        "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
        "if not os.path.isdir(log_dir_tensorboard):\n",
        "    os.makedirs(log_dir_tensorboard)\n",
        "#runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
        "#tb_run = max(runs) + 1 if runs else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6c7ef7f6",
      "metadata": {
        "id": "6c7ef7f6"
      },
      "outputs": [],
      "source": [
        "# Folder creation\n",
        "train_ds_location      = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
        "log_dir_model          = '../../datasets/dsl_data/models/'\n",
        "#run_{}_\n",
        "model_name             = 'epochs_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(args.epochs,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha)\n",
        "checkpoint_path        = '../../datasets/dsl_data/checkpoints/' + model_name\n",
        "#check_point_file_name  = checkpoint_path+'.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a1c4d4b9",
      "metadata": {
        "id": "a1c4d4b9"
      },
      "outputs": [],
      "source": [
        "# If folders to not exist -> create them\n",
        "# This code will not check for the dataset folders, the code above must be executed\n",
        "if not os.path.isdir(log_dir_model):\n",
        "    os.makedirs(log_dir_model)\n",
        "if not os.path.isdir(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb853db5",
      "metadata": {
        "id": "bb853db5"
      },
      "source": [
        " # Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2bf47be3",
      "metadata": {
        "id": "2bf47be3"
      },
      "outputs": [],
      "source": [
        "file_paths = []\n",
        "\n",
        "for filename in os.listdir(train_ds_location):\n",
        "    file_path = os.path.join(train_ds_location, filename)\n",
        "    file_paths.append(file_path)\n",
        "random.shuffle(file_paths)\n",
        "num_test_files = int(len(file_paths) * args.test_percentage)\n",
        "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
        "#not using eval dataset\n",
        "\n",
        "\n",
        "# num_eval_files = num_eval_files\n",
        "\n",
        "# it is shuffled, so i can do this\n",
        "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
        "#train_paths    = file_paths[num_test_files:]\n",
        "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
        "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_paths))\n",
        "print(len(test_paths))\n",
        "print(len(eval_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn1FluvO5Opv",
        "outputId": "592c21a8-09c4-4574-b609-52b75fd36b4c"
      },
      "id": "Qn1FluvO5Opv",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6406\n",
            "1970\n",
            "1478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b36277",
      "metadata": {
        "id": "d8b36277"
      },
      "source": [
        "# Preprocessing data and model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "b8ed3736",
      "metadata": {
        "id": "b8ed3736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce02ccb8-5179-4e6d-e0e3-4c78ac23b446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Shape: (128, 62, 31, 1)\n",
            "Data Shape: (62, 31, 1)\n",
            "Labels: tf.Tensor(\n",
            "[3 6 6 5 3 3 3 0 3 0 4 3 3 3 6 4 3 4 1 3 4 3 2 3 3 1 4 3 5 6 2 3 3 6 3 4 6\n",
            " 4 4 2 3 6 3 0 3 3 4 6 3 5 2 6 3 0 0 6 3 4 4 4 3 4 6 6 3 3 5 5 4 0 1 4 2 3\n",
            " 4 3 6 3 3 1 3 3 1 6 3 4 1 6 6 4 4 5 0 5 0 3 0 3 3 5 3 5 0 4 0 1 6 4 3 4 0\n",
            " 3 3 6 2 4 0 1 4 4 3 6 4 3 4 6 6 5], shape=(128,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
        "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
        "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
        "\n",
        "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
        "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
        "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
        "\n",
        "for example_batch, example_labels in train_ds.take(1):\n",
        "  print('Batch Shape:', example_batch.shape)\n",
        "  print('Data Shape:', example_batch.shape[1:])\n",
        "  print('Labels:', example_labels)\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
        "end_step            = int(len(train_ds) * args.epochs)\n",
        "pruning_params      = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=args.initial_sparsity,\n",
        "        final_sparsity=pr.final_sparsity,\n",
        "        begin_step=begin_step,\n",
        "        end_step=end_step\n",
        "    )\n",
        "}\n",
        "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
        "\n",
        "# model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)\n",
        "# model_name += '.h5'\n",
        "\n",
        "hparams = {\n",
        "'num_units' : num_units,\n",
        "'alpha_rate': args.alpha,\n",
        "'epochs'    : args.epochs,\n",
        "'batch_size': args.batch_size,\n",
        "}\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
        "        use_bias=False, padding='valid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
        "            use_bias=False, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
        "        use_bias=False, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
        "    tf.keras.layers.Softmax()\n",
        "    ])\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "eb8ec704",
      "metadata": {
        "id": "eb8ec704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43655c55-206c-4d9b-88f2-d559d3a0c2f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(62, 31, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_conv2d   (None, 30, 15, 128)      2306      \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_batch_n  (None, 30, 15, 128)      513       \n",
            " ormalization (PruneLowMagni                                     \n",
            " tude)                                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_re_lu (  (None, 30, 15, 128)      1         \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d_  (None, 30, 15, 128)      294914    \n",
            " 1 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_batch_n  (None, 30, 15, 128)      513       \n",
            " ormalization_1 (PruneLowMag                                     \n",
            " nitude)                                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_re_lu_1  (None, 30, 15, 128)      1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d_  (None, 30, 15, 128)      294914    \n",
            " 2 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_batch_n  (None, 30, 15, 128)      513       \n",
            " ormalization_2 (PruneLowMag                                     \n",
            " nitude)                                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_re_lu_2  (None, 30, 15, 128)      1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_global_  (None, 128)              1         \n",
            " average_pooling2d (PruneLow                                     \n",
            " Magnitude)                                                      \n",
            "                                                                 \n",
            " prune_low_magnitude_dense (  (None, 7)                1801      \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_softmax  (None, 7)                1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 595,479\n",
            "Trainable params: 297,735\n",
            "Non-trainable params: 297,744\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
        "print(example_batch.shape[1:])\n",
        "model_for_pruning.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model fitting"
      ],
      "metadata": {
        "id": "dSA36b8J31Um"
      },
      "id": "dSA36b8J31Um"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "69b35a1c",
      "metadata": {
        "scrolled": true,
        "id": "69b35a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c6f082-b4ab-4294-d9f0-c7d634a02e9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous check_point found.\n",
            "Epoch 1/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 1.8973 - sparse_categorical_accuracy: 0.2362\n",
            "Epoch 1: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 67s 368ms/step - loss: 1.8973 - sparse_categorical_accuracy: 0.2362 - val_loss: 7.2949 - val_sparse_categorical_accuracy: 0.2774\n",
            "Epoch 2/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.7680 - sparse_categorical_accuracy: 0.2640\n",
            "Epoch 2: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 1.7689 - sparse_categorical_accuracy: 0.2638 - val_loss: 3.1218 - val_sparse_categorical_accuracy: 0.2747\n",
            "Epoch 3/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.7390 - sparse_categorical_accuracy: 0.2775\n",
            "Epoch 3: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 1.7387 - sparse_categorical_accuracy: 0.2779 - val_loss: 3.4245 - val_sparse_categorical_accuracy: 0.2808\n",
            "Epoch 4/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.7087 - sparse_categorical_accuracy: 0.2975\n",
            "Epoch 4: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 1.7084 - sparse_categorical_accuracy: 0.2978 - val_loss: 2.3135 - val_sparse_categorical_accuracy: 0.2327\n",
            "Epoch 5/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.6662 - sparse_categorical_accuracy: 0.3125\n",
            "Epoch 5: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 1.6664 - sparse_categorical_accuracy: 0.3124 - val_loss: 2.9720 - val_sparse_categorical_accuracy: 0.1245\n",
            "Epoch 6/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.5906 - sparse_categorical_accuracy: 0.3386\n",
            "Epoch 6: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 1.5893 - sparse_categorical_accuracy: 0.3387 - val_loss: 2.4743 - val_sparse_categorical_accuracy: 0.1137\n",
            "Epoch 7/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.5160 - sparse_categorical_accuracy: 0.3766\n",
            "Epoch 7: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 106ms/step - loss: 1.5148 - sparse_categorical_accuracy: 0.3768 - val_loss: 7.6465 - val_sparse_categorical_accuracy: 0.1191\n",
            "Epoch 8/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.4297 - sparse_categorical_accuracy: 0.4211\n",
            "Epoch 8: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 1.4274 - sparse_categorical_accuracy: 0.4219 - val_loss: 3.7043 - val_sparse_categorical_accuracy: 0.1326\n",
            "Epoch 9/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 1.3147 - sparse_categorical_accuracy: 0.4722\n",
            "Epoch 9: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 1.3141 - sparse_categorical_accuracy: 0.4725 - val_loss: 2.6728 - val_sparse_categorical_accuracy: 0.1604\n",
            "Epoch 10/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.2015 - sparse_categorical_accuracy: 0.5183\n",
            "Epoch 10: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 1.1994 - sparse_categorical_accuracy: 0.5197 - val_loss: 5.0901 - val_sparse_categorical_accuracy: 0.1177\n",
            "Epoch 11/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.1213 - sparse_categorical_accuracy: 0.5548\n",
            "Epoch 11: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 104ms/step - loss: 1.1196 - sparse_categorical_accuracy: 0.5562 - val_loss: 2.8833 - val_sparse_categorical_accuracy: 0.1989\n",
            "Epoch 12/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 1.0496 - sparse_categorical_accuracy: 0.5762\n",
            "Epoch 12: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 1.0491 - sparse_categorical_accuracy: 0.5781 - val_loss: 1.6291 - val_sparse_categorical_accuracy: 0.4344\n",
            "Epoch 13/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.9661 - sparse_categorical_accuracy: 0.6181\n",
            "Epoch 13: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 148ms/step - loss: 0.9655 - sparse_categorical_accuracy: 0.6190 - val_loss: 2.2137 - val_sparse_categorical_accuracy: 0.4005\n",
            "Epoch 14/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.8998 - sparse_categorical_accuracy: 0.6472\n",
            "Epoch 14: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.8991 - sparse_categorical_accuracy: 0.6478 - val_loss: 4.1534 - val_sparse_categorical_accuracy: 0.2720\n",
            "Epoch 15/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.8335 - sparse_categorical_accuracy: 0.6755\n",
            "Epoch 15: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.8325 - sparse_categorical_accuracy: 0.6764 - val_loss: 3.8954 - val_sparse_categorical_accuracy: 0.3261\n",
            "Epoch 16/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.7933 - sparse_categorical_accuracy: 0.6914\n",
            "Epoch 16: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.7927 - sparse_categorical_accuracy: 0.6917 - val_loss: 2.2163 - val_sparse_categorical_accuracy: 0.4263\n",
            "Epoch 17/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.7512 - sparse_categorical_accuracy: 0.7103\n",
            "Epoch 17: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.7502 - sparse_categorical_accuracy: 0.7120 - val_loss: 1.1810 - val_sparse_categorical_accuracy: 0.5690\n",
            "Epoch 18/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.6933 - sparse_categorical_accuracy: 0.7353\n",
            "Epoch 18: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.6918 - sparse_categorical_accuracy: 0.7367 - val_loss: 1.3238 - val_sparse_categorical_accuracy: 0.5392\n",
            "Epoch 19/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.6607 - sparse_categorical_accuracy: 0.7478\n",
            "Epoch 19: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 145ms/step - loss: 0.6602 - sparse_categorical_accuracy: 0.7480 - val_loss: 1.1261 - val_sparse_categorical_accuracy: 0.5920\n",
            "Epoch 20/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.6210 - sparse_categorical_accuracy: 0.7625\n",
            "Epoch 20: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.6205 - sparse_categorical_accuracy: 0.7627 - val_loss: 1.5566 - val_sparse_categorical_accuracy: 0.5399\n",
            "Epoch 21/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.5769 - sparse_categorical_accuracy: 0.7805\n",
            "Epoch 21: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.5764 - sparse_categorical_accuracy: 0.7807 - val_loss: 1.2445 - val_sparse_categorical_accuracy: 0.5805\n",
            "Epoch 22/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.5463 - sparse_categorical_accuracy: 0.7953\n",
            "Epoch 22: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.5461 - sparse_categorical_accuracy: 0.7957 - val_loss: 1.3432 - val_sparse_categorical_accuracy: 0.5595\n",
            "Epoch 23/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.5145 - sparse_categorical_accuracy: 0.8111\n",
            "Epoch 23: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 110ms/step - loss: 0.5141 - sparse_categorical_accuracy: 0.8113 - val_loss: 1.0887 - val_sparse_categorical_accuracy: 0.6245\n",
            "Epoch 24/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.4808 - sparse_categorical_accuracy: 0.8241\n",
            "Epoch 24: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.4804 - sparse_categorical_accuracy: 0.8242 - val_loss: 1.3602 - val_sparse_categorical_accuracy: 0.5954\n",
            "Epoch 25/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.4341 - sparse_categorical_accuracy: 0.8398\n",
            "Epoch 25: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 110ms/step - loss: 0.4336 - sparse_categorical_accuracy: 0.8403 - val_loss: 1.5371 - val_sparse_categorical_accuracy: 0.5954\n",
            "Epoch 26/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.4020 - sparse_categorical_accuracy: 0.8546\n",
            "Epoch 26: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.4013 - sparse_categorical_accuracy: 0.8551 - val_loss: 1.3712 - val_sparse_categorical_accuracy: 0.5981\n",
            "Epoch 27/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.3780 - sparse_categorical_accuracy: 0.8701\n",
            "Epoch 27: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 110ms/step - loss: 0.3782 - sparse_categorical_accuracy: 0.8700 - val_loss: 1.4116 - val_sparse_categorical_accuracy: 0.6340\n",
            "Epoch 28/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.3698 - sparse_categorical_accuracy: 0.8711\n",
            "Epoch 28: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.3695 - sparse_categorical_accuracy: 0.8712 - val_loss: 2.8909 - val_sparse_categorical_accuracy: 0.4493\n",
            "Epoch 29/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.3765 - sparse_categorical_accuracy: 0.8645\n",
            "Epoch 29: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 111ms/step - loss: 0.3762 - sparse_categorical_accuracy: 0.8647 - val_loss: 2.9752 - val_sparse_categorical_accuracy: 0.5162\n",
            "Epoch 30/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.3834 - sparse_categorical_accuracy: 0.8578\n",
            "Epoch 30: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 108ms/step - loss: 0.3831 - sparse_categorical_accuracy: 0.8579 - val_loss: 2.9707 - val_sparse_categorical_accuracy: 0.5223\n",
            "Epoch 31/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.3556 - sparse_categorical_accuracy: 0.8672\n",
            "Epoch 31: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.3549 - sparse_categorical_accuracy: 0.8679 - val_loss: 4.8179 - val_sparse_categorical_accuracy: 0.3045\n",
            "Epoch 32/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.3028 - sparse_categorical_accuracy: 0.8975\n",
            "Epoch 32: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 109ms/step - loss: 0.3025 - sparse_categorical_accuracy: 0.8976 - val_loss: 3.8114 - val_sparse_categorical_accuracy: 0.3674\n",
            "Epoch 33/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.2774 - sparse_categorical_accuracy: 0.9101\n",
            "Epoch 33: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.2767 - sparse_categorical_accuracy: 0.9107 - val_loss: 3.9716 - val_sparse_categorical_accuracy: 0.3444\n",
            "Epoch 34/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.2680 - sparse_categorical_accuracy: 0.9134\n",
            "Epoch 34: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.2671 - sparse_categorical_accuracy: 0.9138 - val_loss: 2.5440 - val_sparse_categorical_accuracy: 0.4824\n",
            "Epoch 35/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2641 - sparse_categorical_accuracy: 0.9130\n",
            "Epoch 35: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.2639 - sparse_categorical_accuracy: 0.9131 - val_loss: 1.8836 - val_sparse_categorical_accuracy: 0.5954\n",
            "Epoch 36/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2570 - sparse_categorical_accuracy: 0.9148\n",
            "Epoch 36: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.2567 - sparse_categorical_accuracy: 0.9149 - val_loss: 1.5299 - val_sparse_categorical_accuracy: 0.6204\n",
            "Epoch 37/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.2417 - sparse_categorical_accuracy: 0.9206\n",
            "Epoch 37: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.2435 - sparse_categorical_accuracy: 0.9191 - val_loss: 0.8399 - val_sparse_categorical_accuracy: 0.7422\n",
            "Epoch 38/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.2285 - sparse_categorical_accuracy: 0.9239\n",
            "Epoch 38: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 108ms/step - loss: 0.2297 - sparse_categorical_accuracy: 0.9230 - val_loss: 0.9034 - val_sparse_categorical_accuracy: 0.7124\n",
            "Epoch 39/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2242 - sparse_categorical_accuracy: 0.9250\n",
            "Epoch 39: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.2240 - sparse_categorical_accuracy: 0.9251 - val_loss: 1.4480 - val_sparse_categorical_accuracy: 0.6353\n",
            "Epoch 40/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.2249 - sparse_categorical_accuracy: 0.9257\n",
            "Epoch 40: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.2240 - sparse_categorical_accuracy: 0.9263 - val_loss: 1.3359 - val_sparse_categorical_accuracy: 0.6441\n",
            "Epoch 41/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2282 - sparse_categorical_accuracy: 0.9239\n",
            "Epoch 41: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.2280 - sparse_categorical_accuracy: 0.9240 - val_loss: 0.9552 - val_sparse_categorical_accuracy: 0.7375\n",
            "Epoch 42/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2387 - sparse_categorical_accuracy: 0.9180\n",
            "Epoch 42: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.2385 - sparse_categorical_accuracy: 0.9180 - val_loss: 1.5323 - val_sparse_categorical_accuracy: 0.6448\n",
            "Epoch 43/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.2515 - sparse_categorical_accuracy: 0.9091\n",
            "Epoch 43: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 135ms/step - loss: 0.2520 - sparse_categorical_accuracy: 0.9091 - val_loss: 2.9746 - val_sparse_categorical_accuracy: 0.5217\n",
            "Epoch 44/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2394 - sparse_categorical_accuracy: 0.9158\n",
            "Epoch 44: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.2392 - sparse_categorical_accuracy: 0.9159 - val_loss: 3.4297 - val_sparse_categorical_accuracy: 0.3945\n",
            "Epoch 45/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2268 - sparse_categorical_accuracy: 0.9180\n",
            "Epoch 45: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.2266 - sparse_categorical_accuracy: 0.9180 - val_loss: 2.5005 - val_sparse_categorical_accuracy: 0.4682\n",
            "Epoch 46/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.2020 - sparse_categorical_accuracy: 0.9350\n",
            "Epoch 46: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9351 - val_loss: 7.6316 - val_sparse_categorical_accuracy: 0.2835\n",
            "Epoch 47/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.1766 - sparse_categorical_accuracy: 0.9461\n",
            "Epoch 47: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.1762 - sparse_categorical_accuracy: 0.9463 - val_loss: 3.5163 - val_sparse_categorical_accuracy: 0.4114\n",
            "Epoch 48/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.1741 - sparse_categorical_accuracy: 0.9442\n",
            "Epoch 48: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.1739 - sparse_categorical_accuracy: 0.9443 - val_loss: 2.2140 - val_sparse_categorical_accuracy: 0.5223\n",
            "Epoch 49/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.1750 - sparse_categorical_accuracy: 0.9434\n",
            "Epoch 49: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 142ms/step - loss: 0.1762 - sparse_categorical_accuracy: 0.9429 - val_loss: 0.9886 - val_sparse_categorical_accuracy: 0.7233\n",
            "Epoch 50/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.1930 - sparse_categorical_accuracy: 0.9332\n",
            "Epoch 50: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9337 - val_loss: 1.2034 - val_sparse_categorical_accuracy: 0.7084\n",
            "Epoch 51/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.1865 - sparse_categorical_accuracy: 0.9366\n",
            "Epoch 51: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.1863 - sparse_categorical_accuracy: 0.9366 - val_loss: 1.9695 - val_sparse_categorical_accuracy: 0.5460\n",
            "Epoch 52/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.1818 - sparse_categorical_accuracy: 0.9378\n",
            "Epoch 52: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.1817 - sparse_categorical_accuracy: 0.9379 - val_loss: 1.4162 - val_sparse_categorical_accuracy: 0.6468\n",
            "Epoch 53/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.1426 - sparse_categorical_accuracy: 0.9557\n",
            "Epoch 53: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.1414 - sparse_categorical_accuracy: 0.9561 - val_loss: 0.9033 - val_sparse_categorical_accuracy: 0.7558\n",
            "Epoch 54/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0861 - sparse_categorical_accuracy: 0.9857\n",
            "Epoch 54: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 113ms/step - loss: 0.0860 - sparse_categorical_accuracy: 0.9856 - val_loss: 0.7930 - val_sparse_categorical_accuracy: 0.7463\n",
            "Epoch 55/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0708 - sparse_categorical_accuracy: 0.9931\n",
            "Epoch 55: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.0708 - sparse_categorical_accuracy: 0.9930 - val_loss: 0.9191 - val_sparse_categorical_accuracy: 0.7023\n",
            "Epoch 56/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0647 - sparse_categorical_accuracy: 0.9952\n",
            "Epoch 56: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 112ms/step - loss: 0.0646 - sparse_categorical_accuracy: 0.9952 - val_loss: 0.7713 - val_sparse_categorical_accuracy: 0.7530\n",
            "Epoch 57/100\n",
            "51/51 [==============================] - ETA: 0s - loss: 0.0593 - sparse_categorical_accuracy: 0.9958\n",
            "Epoch 57: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 121ms/step - loss: 0.0593 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.7869 - val_sparse_categorical_accuracy: 0.7558\n",
            "Epoch 58/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0560 - sparse_categorical_accuracy: 0.9962\n",
            "Epoch 58: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 109ms/step - loss: 0.0559 - sparse_categorical_accuracy: 0.9961 - val_loss: 0.7721 - val_sparse_categorical_accuracy: 0.7605\n",
            "Epoch 59/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0534 - sparse_categorical_accuracy: 0.9965\n",
            "Epoch 59: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 108ms/step - loss: 0.0533 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.7647 - val_sparse_categorical_accuracy: 0.7686\n",
            "Epoch 60/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0513 - sparse_categorical_accuracy: 0.9968\n",
            "Epoch 60: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 110ms/step - loss: 0.0514 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.8178 - val_sparse_categorical_accuracy: 0.7618\n",
            "Epoch 61/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0497 - sparse_categorical_accuracy: 0.9967\n",
            "Epoch 61: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0496 - sparse_categorical_accuracy: 0.9967 - val_loss: 1.0161 - val_sparse_categorical_accuracy: 0.7402\n",
            "Epoch 62/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0494 - sparse_categorical_accuracy: 0.9974\n",
            "Epoch 62: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0493 - sparse_categorical_accuracy: 0.9973 - val_loss: 0.9917 - val_sparse_categorical_accuracy: 0.7409\n",
            "Epoch 63/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0481 - sparse_categorical_accuracy: 0.9962\n",
            "Epoch 63: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.0480 - sparse_categorical_accuracy: 0.9963 - val_loss: 0.7887 - val_sparse_categorical_accuracy: 0.7686\n",
            "Epoch 64/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0476 - sparse_categorical_accuracy: 0.9954\n",
            "Epoch 64: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 115ms/step - loss: 0.0475 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.7515 - val_sparse_categorical_accuracy: 0.7876\n",
            "Epoch 65/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0492 - sparse_categorical_accuracy: 0.9928\n",
            "Epoch 65: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0492 - sparse_categorical_accuracy: 0.9928 - val_loss: 0.7459 - val_sparse_categorical_accuracy: 0.7923\n",
            "Epoch 66/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0432 - sparse_categorical_accuracy: 0.9955\n",
            "Epoch 66: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0432 - sparse_categorical_accuracy: 0.9955 - val_loss: 0.7741 - val_sparse_categorical_accuracy: 0.7876\n",
            "Epoch 67/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0416 - sparse_categorical_accuracy: 0.9958\n",
            "Epoch 67: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 109ms/step - loss: 0.0416 - sparse_categorical_accuracy: 0.9958 - val_loss: 0.9366 - val_sparse_categorical_accuracy: 0.7551\n",
            "Epoch 68/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0398 - sparse_categorical_accuracy: 0.9970\n",
            "Epoch 68: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.0395 - sparse_categorical_accuracy: 0.9970 - val_loss: 1.1540 - val_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 69/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0423 - sparse_categorical_accuracy: 0.9942\n",
            "Epoch 69: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.0422 - sparse_categorical_accuracy: 0.9942 - val_loss: 0.8907 - val_sparse_categorical_accuracy: 0.7578\n",
            "Epoch 70/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0366 - sparse_categorical_accuracy: 0.9963\n",
            "Epoch 70: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 109ms/step - loss: 0.0365 - sparse_categorical_accuracy: 0.9964 - val_loss: 0.8267 - val_sparse_categorical_accuracy: 0.7673\n",
            "Epoch 71/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0282 - sparse_categorical_accuracy: 0.9992\n",
            "Epoch 71: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0281 - sparse_categorical_accuracy: 0.9992 - val_loss: 1.0429 - val_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 72/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0263 - sparse_categorical_accuracy: 0.9995\n",
            "Epoch 72: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9995 - val_loss: 0.8975 - val_sparse_categorical_accuracy: 0.7530\n",
            "Epoch 73/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0245 - sparse_categorical_accuracy: 0.9997\n",
            "Epoch 73: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 108ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9997 - val_loss: 0.8402 - val_sparse_categorical_accuracy: 0.7747\n",
            "Epoch 74/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0228 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 74: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.8103 - val_sparse_categorical_accuracy: 0.7835\n",
            "Epoch 75/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0215 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 75: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 110ms/step - loss: 0.0215 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.8249 - val_sparse_categorical_accuracy: 0.7835\n",
            "Epoch 76/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0204 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 76: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0204 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.8731 - val_sparse_categorical_accuracy: 0.7632\n",
            "Epoch 77/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0194 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 77: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 108ms/step - loss: 0.0194 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.9257 - val_sparse_categorical_accuracy: 0.7497\n",
            "Epoch 78/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0183 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 78: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0183 - sparse_categorical_accuracy: 0.9998 - val_loss: 0.9858 - val_sparse_categorical_accuracy: 0.7382\n",
            "Epoch 79/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0175 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 79: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 143ms/step - loss: 0.0175 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.0373 - val_sparse_categorical_accuracy: 0.7300\n",
            "Epoch 80/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0169 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 80: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 107ms/step - loss: 0.0169 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.0952 - val_sparse_categorical_accuracy: 0.7212\n",
            "Epoch 81/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0165 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 81: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.0165 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.1520 - val_sparse_categorical_accuracy: 0.7111\n",
            "Epoch 82/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0163 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 82: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 114ms/step - loss: 0.0163 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.1953 - val_sparse_categorical_accuracy: 0.7077\n",
            "Epoch 83/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0161 - sparse_categorical_accuracy: 0.9998\n",
            "Epoch 83: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9998 - val_loss: 1.2365 - val_sparse_categorical_accuracy: 0.7050\n",
            "Epoch 84/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 84: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 108ms/step - loss: 0.0160 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2585 - val_sparse_categorical_accuracy: 0.7023\n",
            "Epoch 85/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 85: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0159 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2727 - val_sparse_categorical_accuracy: 0.7016\n",
            "Epoch 86/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0160 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 86: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 109ms/step - loss: 0.0159 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.2409 - val_sparse_categorical_accuracy: 0.7070\n",
            "Epoch 87/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0159 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 87: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0159 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.1759 - val_sparse_categorical_accuracy: 0.7212\n",
            "Epoch 88/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0157 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 88: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0157 - sparse_categorical_accuracy: 1.0000 - val_loss: 1.0499 - val_sparse_categorical_accuracy: 0.7436\n",
            "Epoch 89/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0153 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 89: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 111ms/step - loss: 0.0153 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.8817 - val_sparse_categorical_accuracy: 0.7659\n",
            "Epoch 90/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0146 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 90: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 112ms/step - loss: 0.0145 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7541 - val_sparse_categorical_accuracy: 0.7991\n",
            "Epoch 91/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0136 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 91: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0136 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7174 - val_sparse_categorical_accuracy: 0.7997\n",
            "Epoch 92/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0127 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 92: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 5s 109ms/step - loss: 0.0127 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7223 - val_sparse_categorical_accuracy: 0.8011\n",
            "Epoch 93/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0121 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 93: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 109ms/step - loss: 0.0121 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7373 - val_sparse_categorical_accuracy: 0.8045\n",
            "Epoch 94/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0117 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 94: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0117 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7305 - val_sparse_categorical_accuracy: 0.8024\n",
            "Epoch 95/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0114 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 95: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0113 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.7138 - val_sparse_categorical_accuracy: 0.8045\n",
            "Epoch 96/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0110 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 96: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0109 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6912 - val_sparse_categorical_accuracy: 0.8065\n",
            "Epoch 97/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0105 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 97: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 111ms/step - loss: 0.0105 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6713 - val_sparse_categorical_accuracy: 0.8078\n",
            "Epoch 98/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0100 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 98: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 6s 117ms/step - loss: 0.0100 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6577 - val_sparse_categorical_accuracy: 0.8126\n",
            "Epoch 99/100\n",
            "50/51 [============================>.] - ETA: 0s - loss: 0.0095 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 99: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0095 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6508 - val_sparse_categorical_accuracy: 0.8146\n",
            "Epoch 100/100\n",
            "49/51 [===========================>..] - ETA: 0s - loss: 0.0092 - sparse_categorical_accuracy: 1.0000\n",
            "Epoch 100: saving model to ../../datasets/dsl_data/tensorboard_data/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "51/51 [==============================] - 7s 144ms/step - loss: 0.0091 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6472 - val_sparse_categorical_accuracy: 0.8160\n"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=args.initial_learning_rate,\n",
        "    end_learning_rate=args.end_learning_rate,\n",
        "    decay_steps=len(train_ds) * args.epochs,\n",
        ")\n",
        "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
        "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "callbacks = [ tf.keras.callbacks.ModelCheckpoint(filepath=log_dir_tensorboard+model_name,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1),\n",
        "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
        "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+model_name, histogram_freq=1) , hp.KerasCallback(log_dir_tensorboard+model_name, hparams),\n",
        "             ]\n",
        "\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "if os.path.exists(log_dir_tensorboard+model_name+'.ckpt'):\n",
        "    print(\"Checkpoint found, loading...\")\n",
        "    model.load_weights(log_dir_tensorboard+model_name+'.ckpt')\n",
        "    with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"r\") as file:\n",
        "        contents = file.read()\n",
        "        previous_epoch_run = int(contents)\n",
        "        previous_epoch_run = previous_epoch_run\n",
        "    print(\"Restoring from epoch : {}\".format(previous_epoch_run))\n",
        "else:\n",
        "    print(\"No previous check_point found.\")\n",
        "    previous_epoch_run = 0\n",
        "\n",
        "#validation data is test_ds validation_data=val_ds,\n",
        "    \n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=args.epochs, callbacks=callbacks,verbose=1,initial_epoch=previous_epoch_run) #it was valds\n",
        "\n",
        "with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"w\") as file:\n",
        "    file.write(str(args.epochs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "eab63e37",
      "metadata": {
        "id": "eab63e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0d7286-0e32-4bef-e39e-2ef260be3331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 5s 312ms/step - loss: 0.6840 - sparse_categorical_accuracy: 0.8142\n",
            "Training Loss: 0.0091\n",
            "Training Accuracy: 100.00%\n",
            "\n",
            "Validation Loss: 0.6472\n",
            "Validation Accuracy: 81.60%\n",
            "\n",
            "Test Loss: 0.6840\n",
            "Test Accuracy: 81.42%\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "\n",
        "training_loss = history.history['loss'][-1]\n",
        "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "print(f'Training Loss: {training_loss:.4f}')\n",
        "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
        "print()\n",
        "print(f'Validation Loss: {val_loss:.4f}')\n",
        "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
        "print()\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a63692d0",
      "metadata": {
        "id": "a63692d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5b1e17f-f142-4272-b30c-e293d861f2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "with open(log_dir_model+model_name+\".txt\", \"w\") as file:\n",
        "    file.write(model_name)\n",
        "    file.write(\"\\n\")\n",
        "    file.write(\"Execution lasted: \" + str(args.epochs))\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nTraining Loss: {training_loss:.4f}')\n",
        "    file.write(f'\\nTraining Accuracy: {training_accuracy*100.:.2f}%')\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nValidation Loss: {val_loss:.4f}')\n",
        "    file.write(f'\\nValidation Accuracy: {val_accuracy*100.:.2f}%')\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nTest Loss: {test_loss:.4f}')\n",
        "    file.write(f'\\nTest Accuracy: {test_accuracy*100.:.2f}%')\n",
        "    \n",
        "saved_model_dir = f'./saved_models/last_model_used'\n",
        "if not os.path.exists(saved_model_dir):\n",
        "    os.makedirs(saved_model_dir)\n",
        "model.save(saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_dir_model+model_name)\n",
        "with open(log_dir_model+model_name+\".txt\", \"r\") as file:\n",
        "        contents = file.read()\n",
        "        print(contents)\n",
        "    "
      ],
      "metadata": {
        "id": "Ij00I6v5eHGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1143e648-72a5-46e5-ef30-0450baa62320"
      },
      "id": "Ij00I6v5eHGV",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "../../datasets/dsl_data/models/epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "epochs_100_batch_size_128_pruning_initial_step_0.9_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.9_initial_sparsity_0.4_alpha_0.25\n",
            "Execution lasted: 100\n",
            "\n",
            "Training Loss: 0.0091\n",
            "Training Accuracy: 100.00%\n",
            "\n",
            "Validation Loss: 0.6472\n",
            "Validation Accuracy: 81.60%\n",
            "\n",
            "Test Loss: 0.6840\n",
            "Test Accuracy: 81.42%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LIybt8lv4sju",
        "5shtlITI3HCT",
        "d6-1LOFkKaFd",
        "nNVJJQ8Qh42Q",
        "8b824503",
        "8f2ef26e",
        "69c41b90",
        "169533d1",
        "bb853db5",
        "d8b36277"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}