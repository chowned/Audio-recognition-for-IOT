{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import of necessary parts for google drive"
      ],
      "metadata": {
        "id": "LIybt8lv4sju"
      },
      "id": "LIybt8lv4sju"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "t1BIlB1O6eQ8",
        "outputId": "4e31f3dc-49bc-459a-af11-2b737fa293b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "t1BIlB1O6eQ8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "metadata": {
        "id": "Acu8TupveCFE"
      },
      "id": "Acu8TupveCFE",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/datasets/dsl_data/\")"
      ],
      "metadata": {
        "id": "to-3QVHj7uFP"
      },
      "id": "to-3QVHj7uFP",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "EcdlHyi67tOu",
        "outputId": "14700ff6-932a-4783-b166-bcb66b635c46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EcdlHyi67tOu",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t dsl_data.zip\t __MACOSX\t\t __pycache__\n",
            "development.csv  evaluation.csv  preprocessingGoogle.py  sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls drive/MyDrive"
      ],
      "metadata": {
        "id": "KRAfj_Gl62dK",
        "outputId": "b0b1fd82-3557-4ef4-c612-0f876f02065c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "KRAfj_Gl62dK",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'drive/MyDrive': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary install of dep and import libraries"
      ],
      "metadata": {
        "id": "5shtlITI3HCT"
      },
      "id": "5shtlITI3HCT"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install git > /dev/null\n",
        "#!rm ./requiremen*\n",
        "#!rm ./preprocessing*\n",
        "#!ls\n",
        "!pip install -r ipython psutil==5.9.2 sounddevice==0.4.5 scipy==1.9.1 redis==4.3.4 tensorflow==2.10.0 tensorflow-io==0.27.0 cherrypy==18.8.0 paho-mqtt==1.6.1 > /dev/null\n",
        "!pip install -r librosa tensorflow_model_optimization pandas keras tensorflow_io > /dev/null\n",
        "!pip install tensorflow[io] > /dev/null\n",
        "!pip install tensorflow_model_optimization > /dev/null\n",
        "!pip install pydub\n"
      ],
      "metadata": {
        "id": "wKL8ZIyG6rDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59bba6cb-02a9-440f-9fca-f92b5c832737"
      },
      "id": "wKL8ZIyG6rDt",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ipython'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'librosa'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: tensorflow 2.9.2 does not provide the extra 'io'\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a9c863",
      "metadata": {
        "id": "95a9c863"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tensorflow as tf\n",
        "import random\n",
        "#import tensorflow_io as tfio\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import argparse as ap\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/datasets/dsl_data/')\n",
        "\n",
        "\n",
        "import preprocessingGoogle as pr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8596efa2",
      "metadata": {
        "id": "8596efa2"
      },
      "outputs": [],
      "source": [
        "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
        "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip of archive"
      ],
      "metadata": {
        "id": "d6-1LOFkKaFd"
      },
      "id": "d6-1LOFkKaFd"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!unzip -nq ./dsl_data.zip"
      ],
      "metadata": {
        "id": "qNRZRYvbKcya"
      },
      "id": "qNRZRYvbKcya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "lzxQBYDlLl6f"
      },
      "id": "lzxQBYDlLl6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for finding optimal length of audio files"
      ],
      "metadata": {
        "id": "nNVJJQ8Qh42Q"
      },
      "id": "nNVJJQ8Qh42Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def scan_folder(folder):\n",
        "    duration_count = {}\n",
        "    for root, dirs, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                audio = AudioSegment.from_wav(file_path)\n",
        "                duration = len(audio)\n",
        "                if duration in duration_count:\n",
        "                    duration_count[duration] += 1\n",
        "                else:\n",
        "                    duration_count[duration] = 1\n",
        "    return duration_count\n",
        "\n",
        "def create_dataframe(duration_count):\n",
        "    data = {\"Duration of audio file\": list(duration_count.keys()), \n",
        "            \"Number of audio files with that duration\": list(duration_count.values())}\n",
        "    df = pd.DataFrame(data)\n",
        "    df = df.sort_values(by='Number of audio files with that duration', ascending=False)\n",
        "    return df\n",
        "\n",
        "\n",
        "# find the percentage. The duration returned in second is the size that include 1-percentage inside\n",
        "\n",
        "def find_duration(folder_path, percentage_files=0.9):\n",
        "    duration_count = {}\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                file_path = os.path.join(root, file)\n",
        "                audio = AudioSegment.from_wav(file_path)\n",
        "                duration = len(audio) / 1000 #convert from ms to sec\n",
        "                if duration in duration_count:\n",
        "                    duration_count[duration] += 1\n",
        "                else:\n",
        "                    duration_count[duration] = 1\n",
        "    total_files = sum(duration_count.values())\n",
        "    target_files = total_files * percentage_files\n",
        "    current_count = 0\n",
        "    for duration, count in sorted(duration_count.items()):\n",
        "        current_count += count\n",
        "        if current_count >= target_files:\n",
        "            duration = round(duration)\n",
        "            print(f\"Duration of audio that makes {percentage_files*100}% of the files have that duration is: {duration} seconds\")\n",
        "            return duration\n",
        "    \n",
        "\n",
        "\n",
        "folder_path = '../../datasets/dsl_data/audio'\n",
        "duration = find_duration(folder_path)\n",
        "\n",
        "pathPreprocessing = \"../../datasets/dsl_data/preprocessingGoogle.py\"\n",
        "\n",
        "with open(pathPreprocessing, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Update the 4th line\n",
        "lines[10] = \"length_calculated = {}\".format(duration)\n",
        "\n",
        "\n",
        "with open(pathPreprocessing, \"w\") as file:\n",
        "    file.writelines(lines)\n",
        "print(lines)"
      ],
      "metadata": {
        "id": "wZsUS3bvh9X9"
      },
      "id": "wZsUS3bvh9X9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8b824503",
      "metadata": {
        "id": "8b824503"
      },
      "source": [
        " # Preprocessing for Train dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da2cb4f",
      "metadata": {
        "id": "4da2cb4f"
      },
      "outputs": [],
      "source": [
        "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
        "def process_file(file_path, flag):\n",
        "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
        "    if file_path_exists:\n",
        "        new_sr=16000\n",
        "        # identifier care\n",
        "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
        "        identifier = str(int(identifier))\n",
        "        # label constructor\n",
        "        label = \"\"\n",
        "        if flag == 1:\n",
        "            label  += \"_\"\n",
        "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
        "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
        "            label  += action + object\n",
        "        # If no label available, code will just go on\n",
        "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
        "        y, sr = librosa.load('../../datasets/'+file_path)\n",
        "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
        "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
        "        y_truncated = y_truncated[:int(pr.length_calculated*new_sr)] #if longer\n",
        "        target_length = pr.length_calculated * new_sr\n",
        "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
        "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acdacf47",
      "metadata": {
        "scrolled": true,
        "id": "acdacf47"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
        "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
        "\n",
        "folder_path = '../../datasets/dsl_data/'\n",
        "\n",
        "if not os.path.isdir(new_folder_path):\n",
        "    os.makedirs(new_folder_path) # hoping to have write permissions set\n",
        "if not os.listdir(new_folder_path):\n",
        "    with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
        "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCK MICROSOFT AND FUCK THE FUCKING IDEA OF \n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCKING USING \\ FOR PATH!!!!!!!!!!111oneone!!1!!!\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
        "                executor.submit(process_file, file_path, 1)\n",
        "# print(df)\n",
        "print(\"Execution ended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2ef26e",
      "metadata": {
        "id": "8f2ef26e"
      },
      "source": [
        " # Preprocessing for Evaluation dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66b4ba13",
      "metadata": {
        "id": "66b4ba13"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('../../datasets/dsl_data/evaluation.csv', sep=',')\n",
        "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
        "folder_path = '../../datasets/dsl_data/'\n",
        "\n",
        "if not os.path.isdir(new_folder_path):\n",
        "    os.makedirs(new_folder_path)\n",
        "\n",
        "if not os.listdir(new_folder_path):\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
        "                executor.submit(process_file, file_path, 0)\n",
        "\n",
        "print(\"Execution ended\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c41b90",
      "metadata": {
        "id": "69c41b90"
      },
      "source": [
        "# Auto - updating labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0749be3",
      "metadata": {
        "id": "c0749be3"
      },
      "outputs": [],
      "source": [
        "pathPreprocessing = \"../../datasets/dsl_data/preprocessingGoogle.py\"\n",
        "\n",
        "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
        "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
        "distinct_values = df['labels'].unique()\n",
        "\n",
        "result = 'LABELS = ['\n",
        "for value in distinct_values:\n",
        "    result += \"'\" + str(value) + \"', \"\n",
        "\n",
        "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
        "\n",
        "with open(pathPreprocessing, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Update the 4th line\n",
        "lines[3] = result\n",
        "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
        "\n",
        "with open(pathPreprocessing, \"w\") as file:\n",
        "    file.writelines(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c5a54e",
      "metadata": {
        "id": "18c5a54e"
      },
      "source": [
        "# Model creation and fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5698194",
      "metadata": {
        "id": "a5698194"
      },
      "outputs": [],
      "source": [
        "parser = ap.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--batch_size', default=32, type=int, help=\"Choosing batch size default is 32\")\n",
        "parser.add_argument('--initial_learning_rate', default=0.03, type=float, help=\"Choosing initial_learning_rate\")\n",
        "parser.add_argument('--end_learning_rate', default=0.001, type=float, help=\"Choosing end_learning_rate\")\n",
        "parser.add_argument('--epochs', default=50, type=int, help=\"Choosing epochs\")\n",
        "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
        "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
        "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
        "parser.add_argument('--alpha', default=1, type=float, help=\"Choosing alpha\")\n",
        "\n",
        "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")\n",
        "#,'--eval_percentage','0.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5355da28",
      "metadata": {
        "id": "5355da28"
      },
      "source": [
        "Parser arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cbb23f1",
      "metadata": {
        "id": "0cbb23f1"
      },
      "outputs": [],
      "source": [
        "args = parser.parse_args(['--epochs','50','--batch_size','128','--pruning_initial_step','0.9','--initial_learning_rate','0.03','--end_learning_rate','0.001','--alpha','1.0'])\n",
        "# args = parser.parse_args()\n",
        "num_units = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169533d1",
      "metadata": {
        "id": "169533d1"
      },
      "source": [
        "## This part of the code exist to manage all the folders\n",
        "## Please be careful, if the directories tree is not respected, the code will not work properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c6a22f4",
      "metadata": {
        "id": "3c6a22f4"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "795af38c",
      "metadata": {
        "id": "795af38c"
      },
      "outputs": [],
      "source": [
        "# Useful to save tensorboard data\n",
        "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
        "if not os.path.isdir(log_dir_tensorboard):\n",
        "    os.makedirs(log_dir_tensorboard)\n",
        "#runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
        "#tb_run = max(runs) + 1 if runs else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c7ef7f6",
      "metadata": {
        "id": "6c7ef7f6"
      },
      "outputs": [],
      "source": [
        "# Folder creation\n",
        "train_ds_location      = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
        "log_dir_model          = '../../datasets/dsl_data/models/'\n",
        "#run_{}_\n",
        "model_name             = 'epochs_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(args.epochs,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha)\n",
        "checkpoint_path        = '../../datasets/dsl_data/checkpoints/' + model_name\n",
        "#check_point_file_name  = checkpoint_path+'.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c4d4b9",
      "metadata": {
        "id": "a1c4d4b9"
      },
      "outputs": [],
      "source": [
        "# If folders to not exist -> create them\n",
        "# This code will not check for the dataset folders, the code above must be executed\n",
        "if not os.path.isdir(log_dir_model):\n",
        "    os.makedirs(log_dir_model)\n",
        "if not os.path.isdir(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb853db5",
      "metadata": {
        "id": "bb853db5"
      },
      "source": [
        " # Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bf47be3",
      "metadata": {
        "id": "2bf47be3"
      },
      "outputs": [],
      "source": [
        "file_paths = []\n",
        "\n",
        "for filename in os.listdir(train_ds_location):\n",
        "    file_path = os.path.join(train_ds_location, filename)\n",
        "    file_paths.append(file_path)\n",
        "random.shuffle(file_paths)\n",
        "num_test_files = int(len(file_paths) * args.test_percentage)\n",
        "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
        "#not using eval dataset\n",
        "\n",
        "\n",
        "# num_eval_files = num_eval_files\n",
        "\n",
        "# it is shuffled, so i can do this\n",
        "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
        "#train_paths    = file_paths[num_test_files:]\n",
        "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
        "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_paths))\n",
        "print(len(test_paths))\n",
        "print(len(eval_paths))"
      ],
      "metadata": {
        "id": "Qn1FluvO5Opv"
      },
      "id": "Qn1FluvO5Opv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d8b36277",
      "metadata": {
        "id": "d8b36277"
      },
      "source": [
        "# Preprocessing data and model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8ed3736",
      "metadata": {
        "id": "b8ed3736"
      },
      "outputs": [],
      "source": [
        "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
        "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
        "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
        "\n",
        "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
        "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
        "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
        "\n",
        "for example_batch, example_labels in train_ds.take(1):\n",
        "  print('Batch Shape:', example_batch.shape)\n",
        "  print('Data Shape:', example_batch.shape[1:])\n",
        "  print('Labels:', example_labels)\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
        "end_step            = int(len(train_ds) * args.epochs)\n",
        "pruning_params      = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=args.initial_sparsity,\n",
        "        final_sparsity=pr.final_sparsity,\n",
        "        begin_step=begin_step,\n",
        "        end_step=end_step\n",
        "    )\n",
        "}\n",
        "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
        "\n",
        "# model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)\n",
        "# model_name += '.h5'\n",
        "\n",
        "hparams = {\n",
        "'num_units' : num_units,\n",
        "'alpha_rate': args.alpha,\n",
        "'epochs'    : args.epochs,\n",
        "'batch_size': args.batch_size,\n",
        "}\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
        "        use_bias=False, padding='valid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
        "            use_bias=False, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
        "        use_bias=False, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
        "    tf.keras.layers.Softmax()\n",
        "    ])\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb8ec704",
      "metadata": {
        "id": "eb8ec704"
      },
      "outputs": [],
      "source": [
        "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
        "print(example_batch.shape[1:])\n",
        "#model_for_pruning.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model fitting"
      ],
      "metadata": {
        "id": "dSA36b8J31Um"
      },
      "id": "dSA36b8J31Um"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b35a1c",
      "metadata": {
        "scrolled": true,
        "id": "69b35a1c"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=args.initial_learning_rate,\n",
        "    end_learning_rate=args.end_learning_rate,\n",
        "    decay_steps=len(train_ds) * args.epochs,\n",
        ")\n",
        "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
        "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "callbacks = [ tf.keras.callbacks.ModelCheckpoint(filepath=log_dir_tensorboard+model_name,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1),\n",
        "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
        "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+model_name, histogram_freq=1) , hp.KerasCallback(log_dir_tensorboard+model_name, hparams),\n",
        "             ]\n",
        "\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "if os.path.exists(log_dir_tensorboard+model_name+'.ckpt'):\n",
        "    print(\"Checkpoint found, loading...\")\n",
        "    model.load_weights(log_dir_tensorboard+model_name+'.ckpt')\n",
        "    with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"r\") as file:\n",
        "        contents = file.read()\n",
        "        previous_epoch_run = int(contents)\n",
        "        previous_epoch_run = previous_epoch_run\n",
        "    print(\"Restoring from epoch : {}\".format(previous_epoch_run))\n",
        "else:\n",
        "    print(\"No previous check_point found.\")\n",
        "    previous_epoch_run = 0\n",
        "\n",
        "#validation data is test_ds validation_data=val_ds,\n",
        "    \n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=args.epochs, callbacks=callbacks,verbose=1,initial_epoch=previous_epoch_run) #it was valds\n",
        "\n",
        "with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"w\") as file:\n",
        "    file.write(str(args.epochs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab63e37",
      "metadata": {
        "id": "eab63e37"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "\n",
        "training_loss = history.history['loss'][-1]\n",
        "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "print(f'Training Loss: {training_loss:.4f}')\n",
        "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
        "print()\n",
        "print(f'Validation Loss: {val_loss:.4f}')\n",
        "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
        "print()\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63692d0",
      "metadata": {
        "id": "a63692d0"
      },
      "outputs": [],
      "source": [
        "with open(log_dir_model+model_name+\".txt\", \"w\") as file:\n",
        "    file.write(model_name)\n",
        "    file.write(\"\\n\")\n",
        "    file.write(\"Execution lasted: \" + str(args.epochs))\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nTraining Loss: {training_loss:.4f}')\n",
        "    file.write(f'\\nTraining Accuracy: {training_accuracy*100.:.2f}%')\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nValidation Loss: {val_loss:.4f}')\n",
        "    file.write(f'\\nValidation Accuracy: {val_accuracy*100.:.2f}%')\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nTest Loss: {test_loss:.4f}')\n",
        "    file.write(f'\\nTest Accuracy: {test_accuracy*100.:.2f}%')\n",
        "    \n",
        "saved_model_dir = f'./saved_models/last_model_used'\n",
        "if not os.path.exists(saved_model_dir):\n",
        "    os.makedirs(saved_model_dir)\n",
        "model.save(saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_dir_model+model_name)\n",
        "with open(log_dir_model+model_name+\".txt\", \"r\") as file:\n",
        "        contents = file.read()\n",
        "        print(contents)\n",
        "    "
      ],
      "metadata": {
        "id": "Ij00I6v5eHGV"
      },
      "id": "Ij00I6v5eHGV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LIybt8lv4sju",
        "5shtlITI3HCT",
        "8f2ef26e",
        "69c41b90",
        "169533d1",
        "bb853db5"
      ]
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}