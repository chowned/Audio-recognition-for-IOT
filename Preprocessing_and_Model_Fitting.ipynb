{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96908e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow_io as tfio\n",
    "import preprocessing as pr\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff1fb2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Be sure to have tensorboard, this code will be commented in final release\"\n"
     ]
    }
   ],
   "source": [
    "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
    "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276d5fb1",
   "metadata": {},
   "source": [
    " # Preprocessing for Train dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61b7b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
    "def process_file(file_path, flag):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    if file_path_exists:\n",
    "        new_sr=16000\n",
    "        # identifier care\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # label constructor\n",
    "        label = \"\"\n",
    "        if flag == 1:\n",
    "            label  += \"_\"\n",
    "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "            label  += action + object\n",
    "        # If no label available, code will just go on\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
    "        y, sr = librosa.load('../../datasets/'+file_path)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "        y_truncated = y_truncated[:int(4*new_sr)] #if longer\n",
    "        target_length = 4 * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79b955ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path) # hoping to have write permissions set\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 1)\n",
    "# print(df)\n",
    "print(\"Execution ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ed750",
   "metadata": {},
   "source": [
    " # Preprocessing for Evaluation dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42fb0d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/evaluation.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 0)\n",
    "\n",
    "print(\"Execution ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9769ea",
   "metadata": {},
   "source": [
    "# Auto - updating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd14f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
    "distinct_values = df['labels'].unique()\n",
    "\n",
    "result = 'LABELS = ['\n",
    "for value in distinct_values:\n",
    "    result += \"'\" + str(value) + \"', \"\n",
    "\n",
    "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
    "\n",
    "with open(\"preprocessing.py\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Update the 4th line\n",
    "lines[3] = result\n",
    "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
    "\n",
    "with open(\"preprocessing.py\", \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b39255",
   "metadata": {},
   "source": [
    "# Model creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1b70026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, required=False, help='Choosing eval_percentage', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ap.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', default=pr.TRAINING_ARGS['batch_size'], type=int, help=\"Choosing batch size default is 32\")\n",
    "parser.add_argument('--initial_learning_rate', default=pr.TRAINING_ARGS['initial_learning_rate'], type=float, help=\"Choosing initial_learning_rate\")\n",
    "parser.add_argument('--end_learning_rate', default=pr.TRAINING_ARGS['end_learning_rate'], type=float, help=\"Choosing end_learning_rate\")\n",
    "parser.add_argument('--epochs', default=pr.TRAINING_ARGS['epochs'], type=int, help=\"Choosing epochs\")\n",
    "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
    "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
    "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
    "parser.add_argument('--alpha', default=pr.alpha, type=float, help=\"Choosing alpha\")\n",
    "\n",
    "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6430bce",
   "metadata": {},
   "source": [
    "Parser arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a328071",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--epochs','10','--batch_size','32','--initial_learning_rate','0.03','--end_learning_rate','0.03'])\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3499f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_ds_location   = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
    "log_dir_model       = '../../datasets/dsl_data/models/'\n",
    "\n",
    "runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
    "tb_run = max(runs) + 1 if runs else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3eca4e",
   "metadata": {},
   "source": [
    "Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f871d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for filename in os.listdir(train_ds_location):\n",
    "    file_path = os.path.join(train_ds_location, filename)\n",
    "    file_paths.append(file_path)\n",
    "random.shuffle(file_paths)\n",
    "num_test_files = int(len(file_paths) * args.test_percentage)\n",
    "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
    "\n",
    "# it is shuffled, so i can do this\n",
    "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
    "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
    "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa7b6c0",
   "metadata": {},
   "source": [
    "Preprocessing data and model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f69aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 125, 31, 1)\n",
      "Data Shape: (125, 31, 1)\n",
      "Labels: tf.Tensor([5 1 4 3 4 3 4 4 3 1 5 3 2 6 2 3 4 5 0 4 1 3 4 3 3 3 3 6 4 6 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
    "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
    "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
    "\n",
    "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
    "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "\n",
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
    "end_step            = int(len(train_ds) * args.epochs)\n",
    "pruning_params      = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=args.initial_sparsity,\n",
    "        final_sparsity=pr.final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
    "\n",
    "model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)+'.h5'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
    "        use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "            use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "        use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edc51af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      2306      \n",
      " 3 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_3 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_3  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 4 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_4 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_4  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 5 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_5 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_5  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_global_  (None, 128)              1         \n",
      " average_pooling2d_1 (PruneL                                     \n",
      " owMagnitude)                                                    \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_1  (None, 8)                2058      \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_softmax  (None, 8)                1         \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,736\n",
      "Trainable params: 297,864\n",
      "Non-trainable params: 297,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07b50661",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 65/201 [========>.....................] - ETA: 37s - loss: 1.9081 - sparse_categorical_accuracy: 0.2370"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 24\u001b[0m\n\u001b[0;32m     11\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [ tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     12\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mlog_dir_model\u001b[38;5;241m+\u001b[39mmodel_name, \n\u001b[0;32m     13\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m              tfmot\u001b[38;5;241m.\u001b[39msparsity\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mUpdatePruningStep(), \n\u001b[0;32m     19\u001b[0m              keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir_tensorboard\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_epochs_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_batch_size_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pruning_initial_step_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_initial_learning_rate_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_end_learning_rate_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_test_percentage_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pruning_initial_step_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_initial_sparsity_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_alpha_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tb_run,args\u001b[38;5;241m.\u001b[39mepochs,args\u001b[38;5;241m.\u001b[39mbatch_size,args\u001b[38;5;241m.\u001b[39mpruning_initial_step,args\u001b[38;5;241m.\u001b[39minitial_learning_rate,args\u001b[38;5;241m.\u001b[39mend_learning_rate,args\u001b[38;5;241m.\u001b[39mtest_percentage,args\u001b[38;5;241m.\u001b[39mpruning_initial_step,args\u001b[38;5;241m.\u001b[39minitial_sparsity,args\u001b[38;5;241m.\u001b[39malpha), histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     22\u001b[0m model_for_pruning\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_for_pruning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#it was valds\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=args.initial_learning_rate,\n",
    "    end_learning_rate=args.end_learning_rate,\n",
    "    decay_steps=len(train_ds) * args.epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "callbacks = [ tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=log_dir_model+model_name, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    save_freq='epoch'),\n",
    "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+'run_{}_epochs_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(tb_run,args.epochs,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha), histogram_freq=1)]\n",
    "\n",
    "\n",
    "model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "history = model_for_pruning.fit(train_ds, epochs=args.epochs, validation_data=val_ds,callbacks=callbacks) #it was valds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddf7e30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
