{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a9c863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 12:40:57.807802: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-16 12:40:57.873689: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-16 12:40:57.876835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:57.876843: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-16 12:40:57.893647: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-16 12:40:58.317189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.317224: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.317227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 12:40:58.635107: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-01-16 12:40:58.635279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635308: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635376: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635399: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635421: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-01-16 12:40:58.635447: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-01-16 12:40:58.635922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow_io as tfio\n",
    "import preprocessing as pr\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8596efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Be sure to have tensorboard, this code will be commented in final release\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
    "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b824503",
   "metadata": {},
   "source": [
    " # Preprocessing for Train dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4da2cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
    "def process_file(file_path, flag):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    if file_path_exists:\n",
    "        new_sr=16000\n",
    "        # identifier care\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # label constructor\n",
    "        label = \"\"\n",
    "        if flag == 1:\n",
    "            label  += \"_\"\n",
    "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "            label  += action + object\n",
    "        # If no label available, code will just go on\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
    "        y, sr = librosa.load('../../datasets/'+file_path)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "        y_truncated = y_truncated[:int(4*new_sr)] #if longer\n",
    "        target_length = 4 * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdacf47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path) # hoping to have write permissions set\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 1)\n",
    "# print(df)\n",
    "print(\"Execution ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ef26e",
   "metadata": {},
   "source": [
    " # Preprocessing for Evaluation dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b4ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/evaluation.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 0)\n",
    "\n",
    "print(\"Execution ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c41b90",
   "metadata": {},
   "source": [
    "# Auto - updating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0749be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
    "distinct_values = df['labels'].unique()\n",
    "\n",
    "result = 'LABELS = ['\n",
    "for value in distinct_values:\n",
    "    result += \"'\" + str(value) + \"', \"\n",
    "\n",
    "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
    "\n",
    "with open(\"preprocessing.py\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Update the 4th line\n",
    "lines[3] = result\n",
    "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
    "\n",
    "with open(\"preprocessing.py\", \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5a54e",
   "metadata": {},
   "source": [
    "# Model creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5698194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, required=False, help='Choosing eval_percentage', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ap.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', default=pr.TRAINING_ARGS['batch_size'], type=int, help=\"Choosing batch size default is 32\")\n",
    "parser.add_argument('--initial_learning_rate', default=pr.TRAINING_ARGS['initial_learning_rate'], type=float, help=\"Choosing initial_learning_rate\")\n",
    "parser.add_argument('--end_learning_rate', default=pr.TRAINING_ARGS['end_learning_rate'], type=float, help=\"Choosing end_learning_rate\")\n",
    "parser.add_argument('--epochs', default=pr.TRAINING_ARGS['epochs'], type=int, help=\"Choosing epochs\")\n",
    "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
    "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
    "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
    "parser.add_argument('--alpha', default=pr.alpha, type=float, help=\"Choosing alpha\")\n",
    "\n",
    "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355da28",
   "metadata": {},
   "source": [
    "Parser arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cbb23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--epochs','60','--batch_size','64','--pruning_initial_step','0.2','--initial_learning_rate','0.03','--end_learning_rate','0.001'])\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169533d1",
   "metadata": {},
   "source": [
    "## This part of the code exist to manage all the folders\n",
    "## Please be careful, if the directories tree is not respected, the code will not work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6a22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "795af38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to save tensorboard data\n",
    "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
    "if not os.path.isdir(log_dir_tensorboard):\n",
    "    os.makedirs(log_dir_tensorboard)\n",
    "runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
    "tb_run = max(runs) + 1 if runs else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7ef7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "train_ds_location   = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "log_dir_model       = '../../datasets/dsl_data/models/'\n",
    "model_name          = 'run_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(tb_run,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha)\n",
    "checkpoint_path     = '../../datasets/dsl_data/checkpoints/' + model_name+'.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1c4d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folders to not exist -> create them\n",
    "# This code will not check for the dataset folders, the code above must be executed\n",
    "if not os.path.isdir(log_dir_model):\n",
    "    os.makedirs(log_dir_model)\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb853db5",
   "metadata": {},
   "source": [
    "Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bf47be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for filename in os.listdir(train_ds_location):\n",
    "    file_path = os.path.join(train_ds_location, filename)\n",
    "    file_paths.append(file_path)\n",
    "random.shuffle(file_paths)\n",
    "num_test_files = int(len(file_paths) * args.test_percentage)\n",
    "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
    "\n",
    "# it is shuffled, so i can do this\n",
    "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
    "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
    "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b36277",
   "metadata": {},
   "source": [
    "Preprocessing data and model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ed3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (64, 125, 31, 1)\n",
      "Data Shape: (125, 31, 1)\n",
      "Labels: tf.Tensor(\n",
      "[4 1 3 3 5 3 4 6 4 2 4 4 5 6 3 3 6 3 0 4 5 1 2 1 6 2 3 3 5 0 5 5 4 4 5 2 3\n",
      " 3 4 5 4 4 4 6 6 2 4 3 0 4 5 5 3 0 6 4 3 5 4 0 6 4 2 1], shape=(64,), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 12:40:59.537357: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
    "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
    "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
    "\n",
    "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
    "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "\n",
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
    "end_step            = int(len(train_ds) * args.epochs)\n",
    "pruning_params      = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=args.initial_sparsity,\n",
    "        final_sparsity=pr.final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
    "\n",
    "#model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)\n",
    "model_name += '.h5'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
    "        use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "            use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "        use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb8ec704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d   (None, 62, 15, 128)      2306      \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization (PruneLowMagni                                     \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu (  (None, 62, 15, 128)      1         \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_1 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_1  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_2 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_2  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_global_  (None, 128)              1         \n",
      " average_pooling2d (PruneLow                                     \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 7)                1801      \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_softmax  (None, 7)                1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,479\n",
      "Trainable params: 297,735\n",
      "Non-trainable params: 297,744\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
    "# model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b35a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "100/101 [============================>.] - ETA: 0s - loss: 1.8421 - sparse_categorical_accuracy: 0.2534\n",
      "Epoch 1: saving model to ../../datasets/dsl_data/tensorboard_data/run_6_batch_size_64_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Please specify a non-directory filepath for ModelCheckpoint. Filepath used is an existing directory: ../../datasets/dsl_data/tensorboard_data/run_6_batch_size_64_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m model_for_pruning\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#if os.path.exists(checkpoint_path):\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#    print(\"Checkpoint found, loading...\")\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#    model_for_pruning.load_weights(checkpoint_path)\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_for_pruning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#it was valds\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/callbacks.py:1556\u001b[0m, in \u001b[0;36mModelCheckpoint._save_model\u001b[0;34m(self, epoch, batch, logs)\u001b[0m\n\u001b[1;32m   1554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_remove_file()\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m:  \u001b[38;5;66;03m# h5py 3.x\u001b[39;00m\n\u001b[0;32m-> 1556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m   1557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify a non-directory filepath for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelCheckpoint. Filepath used is an existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1560\u001b[0m     )\n\u001b[1;32m   1561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# h5py 2.x\u001b[39;00m\n\u001b[1;32m   1562\u001b[0m     \u001b[38;5;66;03m# `e.errno` appears to be `None` so checking the content of\u001b[39;00m\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;66;03m# `e.args[0]`.\u001b[39;00m\n\u001b[1;32m   1564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mlower():\n",
      "\u001b[0;31mOSError\u001b[0m: Please specify a non-directory filepath for ModelCheckpoint. Filepath used is an existing directory: ../../datasets/dsl_data/tensorboard_data/run_6_batch_size_64_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1.h5"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=args.initial_learning_rate,\n",
    "    end_learning_rate=args.end_learning_rate,\n",
    "    decay_steps=len(train_ds) * args.epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "callbacks = [ tf.keras.callbacks.ModelCheckpoint(filepath=log_dir_tensorboard+model_name,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1),\n",
    "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+model_name, histogram_freq=1)]\n",
    "\n",
    "\n",
    "model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "#if os.path.exists(checkpoint_path):\n",
    "#    print(\"Checkpoint found, loading...\")\n",
    "#    model_for_pruning.load_weights(checkpoint_path)\n",
    "history = model_for_pruning.fit(train_ds, epochs=args.epochs, validation_data=val_ds,callbacks=callbacks,verbose=1) #it was valds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab63e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63692d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
