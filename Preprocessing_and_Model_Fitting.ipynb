{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19768879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow_io as tfio\n",
    "import preprocessing as pr\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03aecc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Be sure to have tensorboard, this code will be commented in final release\"\n"
     ]
    }
   ],
   "source": [
    "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
    "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564b59c",
   "metadata": {},
   "source": [
    " # Preprocessing for Train dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dee14d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "###############\n",
    "def process_file(file_path):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    # print(file_path)\n",
    "    # print(file_path_exists)\n",
    "    if file_path_exists:\n",
    "        new_sr=16000\n",
    "        # print(file_path)\n",
    "\n",
    "        # identifier care\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # print(\"\\n \"+identifier)\n",
    "\n",
    "        # label constructor\n",
    "\n",
    "        action = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "        object = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "        label  = action + object\n",
    "\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + \"_\" + label + '.wav')\n",
    "\n",
    "        # print(file_path)\n",
    "        y, sr = librosa.load('../../datasets/'+file_path)\n",
    "        # print(\"here\")\n",
    "        # print(y)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "        # print(y_truncated)\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "\n",
    "        y_truncated = y_truncated[:int(4*new_sr)] #if longer\n",
    "\n",
    "        target_length = 4 * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) #padding, if shorter\n",
    "\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path) #hoping to have write permissions set\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor: #multi-threaded beast :)\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            # print(dirpath)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                # print(file_path)\n",
    "                # process_file(file_path)\n",
    "                executor.submit(process_file, file_path)\n",
    "\n",
    "# print(df)\n",
    "print(\"Execution ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50d3c8",
   "metadata": {},
   "source": [
    " # Preprocessing for Evaluation dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6f21fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
    "\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path) #hoping to have write permissions set\n",
    "\n",
    "def process_file(file_path):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    # print(file_path_exists)\n",
    "    if file_path_exists:\n",
    "        new_sr=16000\n",
    "        # print(file_path)\n",
    "\n",
    "        # identifier care\n",
    "\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # print(\"\\n \" + identifier)\n",
    "\n",
    "        # label constructor\n",
    "\n",
    "        # action = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "        # object = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "        # label  = action + object\n",
    "\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + '.wav')\n",
    "\n",
    "        y, sr = librosa.load('../../datasets/'+file_path)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=10, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "\n",
    "        y_truncated = y_truncated[:int(4*new_sr)]\n",
    "\n",
    "        target_length = 4 * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) #padding\n",
    "        # print(new_file_path)\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')\n",
    "\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor: #multi-threaded beast :)\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            # print(dirpath)\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                # print(file_path)\n",
    "                executor.submit(process_file, file_path)\n",
    "\n",
    "# print(df)\n",
    "print(\"Execution ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c3970",
   "metadata": {},
   "source": [
    "# Auto - updating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "811c3c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "\n",
    "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
    "\n",
    "distinct_values = df['labels'].unique()\n",
    "\n",
    "# print(distinct_values)\n",
    "\n",
    "result = 'LABELS = ['\n",
    "for value in distinct_values:\n",
    "    result += \"'\" + str(value) + \"', \"\n",
    "\n",
    "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
    "# result += ']\\n'\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(\"preprocessing.py\", \"r\") as file:\n",
    "    # Read the contents of the file into a list\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Update the 3rd line\n",
    "lines[3] = result\n",
    "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(\"preprocessing.py\", \"w\") as file:\n",
    "    # Write the updated lines back to the file\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb60768",
   "metadata": {},
   "source": [
    "# Model creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e18dc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, required=False, help='Choosing eval_percentage', metavar=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ap.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', default=pr.TRAINING_ARGS['batch_size'], type=int, help=\"Choosing batch size default is 32\")\n",
    "parser.add_argument('--initial_learning_rate', default=pr.TRAINING_ARGS['initial_learning_rate'], type=float, help=\"Choosing initial_learning_rate\")\n",
    "parser.add_argument('--end_learning_rate', default=pr.TRAINING_ARGS['end_learning_rate'], type=float, help=\"Choosing end_learning_rate\")\n",
    "parser.add_argument('--epochs', default=pr.TRAINING_ARGS['epochs'], type=int, help=\"Choosing epochs\")\n",
    "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
    "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
    "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
    "parser.add_argument('--alpha', default=pr.alpha, type=float, help=\"Choosing alpha\")\n",
    "\n",
    "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499e2766",
   "metadata": {},
   "source": [
    "Parser arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920ae880",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--batch_size','32','--initial_learning_rate','0.03','--end_learning_rate','0.03','--epochs','5'])\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e50bc2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# train_ds_location = './Train_Dataset_Truncated/'\n",
    "# eval_ds_location  = './Test_Dataset_Truncated/'\n",
    "\n",
    "train_ds_location   = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "#eval_ds_location    = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
    "eval_ds_location    = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "\n",
    "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
    "log_dir_model       = '../../datasets/dsl_data/models/'\n",
    "\n",
    "runs = [int(d.split('_')[-1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
    "tb_run = max(runs) + 1 if runs else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61132c2",
   "metadata": {},
   "source": [
    "Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c8f7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for filename in os.listdir(train_ds_location):\n",
    "    file_path = os.path.join(train_ds_location, filename)\n",
    "    file_paths.append(file_path)\n",
    "random.shuffle(file_paths)\n",
    "#test_percentage = 0.2\n",
    "num_test_files = int(len(file_paths) * args.test_percentage)\n",
    "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
    "\n",
    "#args.eval_percentage\n",
    "# it is shuffled, so i can do this\n",
    "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
    "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
    "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d54893",
   "metadata": {},
   "source": [
    "Preprocessing data and model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3b43c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 125, 31, 1)\n",
      "Data Shape: (125, 31, 1)\n",
      "Labels: tf.Tensor([5 1 4 3 4 3 4 4 3 1 5 3 2 6 2 3 4 5 0 4 1 3 4 3 3 3 3 6 4 6 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
    "val_ds         = tf.data.Dataset.list_files(eval_ds_location)\n",
    "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
    "\n",
    "#batch_size = pr.TRAINING_ARGS['batch_size']\n",
    "#epochs = pr.TRAINING_ARGS['epochs']\n",
    "\n",
    "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
    "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "\n",
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
    "end_step            = int(len(train_ds) * args.epochs)\n",
    "pruning_params      = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=args.initial_sparsity,\n",
    "        final_sparsity=pr.final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
    "\n",
    "model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)+'.h5'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
    "        use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "            use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "        use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4e3e960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d   (None, 62, 15, 128)      2306      \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization (PruneLowMagni                                     \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu (  (None, 62, 15, 128)      1         \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_1 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_1  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_2 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_2  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_global_  (None, 128)              1         \n",
      " average_pooling2d (PruneLow                                     \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 8)                2058      \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_softmax  (None, 8)                1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,736\n",
      "Trainable params: 297,864\n",
      "Non-trainable params: 297,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd861187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.8369 - sparse_categorical_accuracy: 0.2563"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nNewRandomAccessFile failed to Create/Open: ..\\..\\datasets\\dsl_data\\Train_Dataset_Truncated\\ : Impossibile trovare il percorso specificato.\r\n; No such process\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_test_function_6510]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 27\u001b[0m\n\u001b[0;32m     14\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [ tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     15\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mlog_dir_model\u001b[38;5;241m+\u001b[39mmodel_name, \n\u001b[0;32m     16\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m              tfmot\u001b[38;5;241m.\u001b[39msparsity\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mUpdatePruningStep(), \n\u001b[0;32m     22\u001b[0m              keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mTensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir_tensorboard\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_epochs_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_batch_size_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pruning_initial_step_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_initial_learning_rate_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_end_learning_rate_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_test_percentage_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pruning_initial_step_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_initial_sparsity_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_alpha_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tb_run,args\u001b[38;5;241m.\u001b[39mepochs,args\u001b[38;5;241m.\u001b[39mbatch_size,args\u001b[38;5;241m.\u001b[39mpruning_initial_step,args\u001b[38;5;241m.\u001b[39minitial_learning_rate,args\u001b[38;5;241m.\u001b[39mend_learning_rate,args\u001b[38;5;241m.\u001b[39mtest_percentage,args\u001b[38;5;241m.\u001b[39mpruning_initial_step,args\u001b[38;5;241m.\u001b[39minitial_sparsity,args\u001b[38;5;241m.\u001b[39malpha), histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m     25\u001b[0m model_for_pruning\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mloss, optimizer\u001b[38;5;241m=\u001b[39moptimizer, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[1;32m---> 27\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_for_pruning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#it was valds\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# history = model_for_pruning.fit(train_ds, epochs=epochs, callbacks=callbacks) #it was valds\u001b[39;00m\n\u001b[0;32m     31\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model_for_pruning\u001b[38;5;241m.\u001b[39mevaluate(test_ds)\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Graph execution error:\n\nNewRandomAccessFile failed to Create/Open: ..\\..\\datasets\\dsl_data\\Train_Dataset_Truncated\\ : Impossibile trovare il percorso specificato.\r\n; No such process\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]] [Op:__inference_test_function_6510]"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "#initial_learning_rate = pr.TRAINING_ARGS['initial_learning_rate']\n",
    "#end_learning_rate = pr.TRAINING_ARGS['end_learning_rate']\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=args.initial_learning_rate,\n",
    "    end_learning_rate=args.end_learning_rate,\n",
    "    decay_steps=len(train_ds) * args.epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "# callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n",
    "callbacks = [ tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=log_dir_model+model_name, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    save_freq='epoch'),\n",
    "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+'run_{}_epochs_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(tb_run,args.epochs,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha), histogram_freq=1)]\n",
    "\n",
    "\n",
    "model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "history = model_for_pruning.fit(train_ds, epochs=args.epochs, validation_data=val_ds,callbacks=callbacks) #it was valds\n",
    "# history = model_for_pruning.fit(train_ds, epochs=epochs, callbacks=callbacks) #it was valds\n",
    "\n",
    "\n",
    "test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
