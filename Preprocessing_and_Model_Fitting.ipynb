{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95a9c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow_io as tfio\n",
    "import preprocessing as pr\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8596efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Be sure to have tensorboard, this code will be commented in final release\"\n"
     ]
    }
   ],
   "source": [
    "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
    "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b824503",
   "metadata": {},
   "source": [
    " # Preprocessing for Train dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da2cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
    "def process_file(file_path, flag):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    if file_path_exists:\n",
    "        new_sr=16000\n",
    "        # identifier care\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # label constructor\n",
    "        label = \"\"\n",
    "        if flag == 1:\n",
    "            label  += \"_\"\n",
    "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "            label  += action + object\n",
    "        # If no label available, code will just go on\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
    "        y, sr = librosa.load('../../datasets/'+file_path)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "        y_truncated = y_truncated[:int(4*new_sr)] #if longer\n",
    "        target_length = 4 * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acdacf47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path) # hoping to have write permissions set\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCK MICROSOFT AND FUCK THE FUCKING IDEA OF \n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCKING USING \\ FOR PATH!!!!!!!!!!111oneone!!1!!!\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 1)\n",
    "# print(df)\n",
    "print(\"Execution ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ef26e",
   "metadata": {},
   "source": [
    " # Preprocessing for Evaluation dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66b4ba13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/evaluation.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 0)\n",
    "\n",
    "print(\"Execution ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c41b90",
   "metadata": {},
   "source": [
    "# Auto - updating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0749be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
    "distinct_values = df['labels'].unique()\n",
    "\n",
    "result = 'LABELS = ['\n",
    "for value in distinct_values:\n",
    "    result += \"'\" + str(value) + \"', \"\n",
    "\n",
    "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
    "\n",
    "with open(\"preprocessing.py\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Update the 4th line\n",
    "lines[3] = result\n",
    "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
    "\n",
    "with open(\"preprocessing.py\", \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c5a54e",
   "metadata": {},
   "source": [
    "# Model creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5698194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, required=False, help='Choosing eval_percentage', metavar=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ap.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', default=pr.TRAINING_ARGS['batch_size'], type=int, help=\"Choosing batch size default is 32\")\n",
    "parser.add_argument('--initial_learning_rate', default=pr.TRAINING_ARGS['initial_learning_rate'], type=float, help=\"Choosing initial_learning_rate\")\n",
    "parser.add_argument('--end_learning_rate', default=pr.TRAINING_ARGS['end_learning_rate'], type=float, help=\"Choosing end_learning_rate\")\n",
    "parser.add_argument('--epochs', default=pr.TRAINING_ARGS['epochs'], type=int, help=\"Choosing epochs\")\n",
    "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
    "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
    "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
    "parser.add_argument('--alpha', default=pr.alpha, type=float, help=\"Choosing alpha\")\n",
    "\n",
    "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355da28",
   "metadata": {},
   "source": [
    "Parser arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0cbb23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--epochs','20','--batch_size','32','--pruning_initial_step','0.2','--initial_learning_rate','0.03','--end_learning_rate','0.001'])\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169533d1",
   "metadata": {},
   "source": [
    "## This part of the code exist to manage all the folders\n",
    "## Please be careful, if the directories tree is not respected, the code will not work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c6a22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "795af38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to save tensorboard data\n",
    "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
    "if not os.path.isdir(log_dir_tensorboard):\n",
    "    os.makedirs(log_dir_tensorboard)\n",
    "#runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
    "#tb_run = max(runs) + 1 if runs else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c7ef7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "train_ds_location      = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "log_dir_model          = '../../datasets/dsl_data/models/'\n",
    "#run_{}_\n",
    "model_name             = 'batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha)\n",
    "checkpoint_path        = '../../datasets/dsl_data/checkpoints/' + model_name\n",
    "#check_point_file_name  = checkpoint_path+'.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a1c4d4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If folders to not exist -> create them\n",
    "# This code will not check for the dataset folders, the code above must be executed\n",
    "if not os.path.isdir(log_dir_model):\n",
    "    os.makedirs(log_dir_model)\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb853db5",
   "metadata": {},
   "source": [
    "Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bf47be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for filename in os.listdir(train_ds_location):\n",
    "    file_path = os.path.join(train_ds_location, filename)\n",
    "    file_paths.append(file_path)\n",
    "random.shuffle(file_paths)\n",
    "num_test_files = int(len(file_paths) * args.test_percentage)\n",
    "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
    "\n",
    "# it is shuffled, so i can do this\n",
    "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
    "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
    "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b36277",
   "metadata": {},
   "source": [
    "Preprocessing data and model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8ed3736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 125, 31, 1)\n",
      "Data Shape: (125, 31, 1)\n",
      "Labels: tf.Tensor([5 1 4 3 4 3 4 4 3 1 5 3 2 6 2 3 4 5 0 4 1 3 4 3 3 3 3 6 4 6 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
    "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
    "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
    "\n",
    "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
    "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "\n",
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
    "end_step            = int(len(train_ds) * args.epochs)\n",
    "pruning_params      = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=args.initial_sparsity,\n",
    "        final_sparsity=pr.final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
    "\n",
    "# model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)\n",
    "# model_name += '.h5'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
    "        use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "            use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "        use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb8ec704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 31, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      2306      \n",
      " 3 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_3 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_3  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 4 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_4 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_4  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 5 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_5 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_5  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_global_  (None, 128)              1         \n",
      " average_pooling2d_1 (PruneL                                     \n",
      " owMagnitude)                                                    \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_1  (None, 8)                2058      \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_softmax  (None, 8)                1         \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,736\n",
      "Trainable params: 297,864\n",
      "Non-trainable params: 297,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
    "print(example_batch.shape[1:])\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b35a1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous check_point found.\n",
      "Epoch 1/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.8405 - sparse_categorical_accuracy: 0.2591\n",
      "Epoch 1: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 68s 323ms/step - loss: 1.8405 - sparse_categorical_accuracy: 0.2591 - val_loss: 12.3345 - val_sparse_categorical_accuracy: 0.2530\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.7797 - sparse_categorical_accuracy: 0.2696\n",
      "Epoch 2: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 285ms/step - loss: 1.7797 - sparse_categorical_accuracy: 0.2696 - val_loss: 27.2530 - val_sparse_categorical_accuracy: 0.2530\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.7451 - sparse_categorical_accuracy: 0.2727\n",
      "Epoch 3: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 284ms/step - loss: 1.7451 - sparse_categorical_accuracy: 0.2727 - val_loss: 9.8844 - val_sparse_categorical_accuracy: 0.0480\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.7128 - sparse_categorical_accuracy: 0.2868\n",
      "Epoch 4: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 283ms/step - loss: 1.7128 - sparse_categorical_accuracy: 0.2868 - val_loss: 2.1355 - val_sparse_categorical_accuracy: 0.0866\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.6212 - sparse_categorical_accuracy: 0.3322\n",
      "Epoch 5: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 283ms/step - loss: 1.6212 - sparse_categorical_accuracy: 0.3322 - val_loss: 2.0894 - val_sparse_categorical_accuracy: 0.1509\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.5352 - sparse_categorical_accuracy: 0.3731\n",
      "Epoch 6: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 282ms/step - loss: 1.5352 - sparse_categorical_accuracy: 0.3731 - val_loss: 1.9527 - val_sparse_categorical_accuracy: 0.1982\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.4296 - sparse_categorical_accuracy: 0.4251\n",
      "Epoch 7: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 282ms/step - loss: 1.4296 - sparse_categorical_accuracy: 0.4251 - val_loss: 2.2176 - val_sparse_categorical_accuracy: 0.2321\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.3468 - sparse_categorical_accuracy: 0.4549\n",
      "Epoch 8: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 285ms/step - loss: 1.3468 - sparse_categorical_accuracy: 0.4549 - val_loss: 1.5442 - val_sparse_categorical_accuracy: 0.3972\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.2655 - sparse_categorical_accuracy: 0.4952\n",
      "Epoch 9: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 285ms/step - loss: 1.2655 - sparse_categorical_accuracy: 0.4952 - val_loss: 1.5180 - val_sparse_categorical_accuracy: 0.4154\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.1870 - sparse_categorical_accuracy: 0.5245\n",
      "Epoch 10: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 61s 302ms/step - loss: 1.1870 - sparse_categorical_accuracy: 0.5245 - val_loss: 1.5455 - val_sparse_categorical_accuracy: 0.3870\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.1153 - sparse_categorical_accuracy: 0.5613\n",
      "Epoch 11: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 58s 291ms/step - loss: 1.1153 - sparse_categorical_accuracy: 0.5613 - val_loss: 1.2540 - val_sparse_categorical_accuracy: 0.4675\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 1.0527 - sparse_categorical_accuracy: 0.5834\n",
      "Epoch 12: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 284ms/step - loss: 1.0527 - sparse_categorical_accuracy: 0.5834 - val_loss: 1.4525 - val_sparse_categorical_accuracy: 0.4215\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 0.9995 - sparse_categorical_accuracy: 0.6076\n",
      "Epoch 13: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 283ms/step - loss: 0.9995 - sparse_categorical_accuracy: 0.6076 - val_loss: 1.3492 - val_sparse_categorical_accuracy: 0.4520\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 0.9452 - sparse_categorical_accuracy: 0.6332\n",
      "Epoch 14: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 283ms/step - loss: 0.9452 - sparse_categorical_accuracy: 0.6332 - val_loss: 1.2780 - val_sparse_categorical_accuracy: 0.4689\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 0.8969 - sparse_categorical_accuracy: 0.6563\n",
      "Epoch 15: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 283ms/step - loss: 0.8969 - sparse_categorical_accuracy: 0.6563 - val_loss: 1.1742 - val_sparse_categorical_accuracy: 0.5169\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 0.8560 - sparse_categorical_accuracy: 0.6767\n",
      "Epoch 16: saving model to ../../datasets/dsl_data/tensorboard_data\\batch_size_32_pruning_initial_step_0.2_initial_learning_rate_0.03_end_learning_rate_0.001_test_percentage_0.2_pruning_initial_step_0.2_initial_sparsity_0.4_alpha_1\n",
      "201/201 [==============================] - 57s 283ms/step - loss: 0.8560 - sparse_categorical_accuracy: 0.6767 - val_loss: 1.0927 - val_sparse_categorical_accuracy: 0.5487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "201/201 [==============================] - ETA: 0s - loss: 0.8196 - sparse_categorical_accuracy: 0.6892"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=args.initial_learning_rate,\n",
    "    end_learning_rate=args.end_learning_rate,\n",
    "    decay_steps=len(train_ds) * args.epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "callbacks = [ tf.keras.callbacks.ModelCheckpoint(filepath=log_dir_tensorboard+model_name,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1),\n",
    "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+model_name, histogram_freq=1)]\n",
    "\n",
    "\n",
    "model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "if os.path.exists(log_dir_tensorboard+model_name+'.ckpt'):\n",
    "    print(\"Checkpoint found, loading...\")\n",
    "    model_for_pruning.load_weights(log_dir_tensorboard+model_name+'.ckpt')\n",
    "    with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"r\") as file:\n",
    "        contents = file.read()\n",
    "        previous_epoch_run = int(contents)\n",
    "        previous_epoch_run = previous_epoch_run\n",
    "    print(\"Restoring from epoch : {}\".format(previous_epoch_run))\n",
    "else:\n",
    "    print(\"No previous check_point found.\")\n",
    "    previous_epoch_run = 0\n",
    "    \n",
    "history = model_for_pruning.fit(train_ds, epochs=args.epochs, validation_data=val_ds,callbacks=callbacks,verbose=1,initial_epoch=previous_epoch_run) #it was valds\n",
    "\n",
    "with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"w\") as file:\n",
    "    file.write(str(args.epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab63e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63692d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(log_dir_model+model_name+\".txt\", \"w\") as file:\n",
    "    file.write(model_name)\n",
    "    file.write(\"\\n\")\n",
    "    file.write(\"Execution lasted: \" + str(args.epochs))\n",
    "    file.write(\"\\n\")\n",
    "    file.write(f'\\nTraining Loss: {training_loss:.4f}')\n",
    "    file.write(f'\\nTraining Accuracy: {training_accuracy*100.:.2f}%')\n",
    "    file.write(\"\\n\")\n",
    "    file.write(f'\\nValidation Loss: {val_loss:.4f}')\n",
    "    file.write(f'\\nValidation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "    file.write(\"\\n\")\n",
    "    file.write(f'\\nTest Loss: {test_loss:.4f}')\n",
    "    file.write(f'\\nTest Accuracy: {test_accuracy*100.:.2f}%')\n",
    "    \n",
    "saved_model_dir = f'./saved_models/last_model_used'\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "model_for_pruning.save(saved_model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
