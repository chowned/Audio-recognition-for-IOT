{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import of necessary parts for google drive"
      ],
      "metadata": {
        "id": "LIybt8lv4sju"
      },
      "id": "LIybt8lv4sju"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "id": "t1BIlB1O6eQ8",
        "outputId": "d51e1b24-1da5-4484-bf99-673f6083b641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "t1BIlB1O6eQ8",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "metadata": {
        "id": "Acu8TupveCFE"
      },
      "id": "Acu8TupveCFE",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"drive/MyDrive/datasets/dsl_data/\")"
      ],
      "metadata": {
        "id": "to-3QVHj7uFP"
      },
      "id": "to-3QVHj7uFP",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "EcdlHyi67tOu",
        "outputId": "bc5c7195-1411-420f-cb26-016537282c15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EcdlHyi67tOu",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t __MACOSX\t\t saved_models\n",
            "checkpoints\t models\t\t\t tensorboard_data\n",
            "development.csv  preprocessingGoogle.py  Test_Dataset_Truncated\n",
            "dsl_data.zip\t __pycache__\t\t Train_Dataset_Truncated\n",
            "evaluation.csv\t sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary install of dep and import libraries"
      ],
      "metadata": {
        "id": "5shtlITI3HCT"
      },
      "id": "5shtlITI3HCT"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install git > /dev/null\n",
        "#!rm ./requiremen*\n",
        "#!rm ./preprocessing*\n",
        "#!ls\n",
        "!pip install -r ipython psutil==5.9.2 sounddevice==0.4.5 scipy==1.9.1 redis==4.3.4 tensorflow==2.10.0 tensorflow-io==0.27.0 cherrypy==18.8.0 paho-mqtt==1.6.1 > /dev/null\n",
        "!pip install -r librosa tensorflow_model_optimization pandas keras tensorflow_io > /dev/null\n",
        "!pip install tensorflow[io] > /dev/null\n",
        "!pip install tensorflow_model_optimization > /dev/null\n",
        "!pip install pydub\n"
      ],
      "metadata": {
        "id": "wKL8ZIyG6rDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb3652a-473a-497c-d9e8-4160897448b6"
      },
      "id": "wKL8ZIyG6rDt",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ipython'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'librosa'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: tensorflow 2.9.2 does not provide the extra 'io'\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "95a9c863",
      "metadata": {
        "id": "95a9c863"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "import librosa\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import tensorflow as tf\n",
        "import random\n",
        "#import tensorflow_io as tfio\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "import argparse as ap\n",
        "\n",
        "import sys\n",
        "sys.path.append('drive/MyDrive/datasets/dsl_data/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8596efa2",
      "metadata": {
        "id": "8596efa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eebf6bc-2ced-442b-c199-372e2463507b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Be sure to have tensorboard, this code will be commented in final release\n"
          ]
        }
      ],
      "source": [
        "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
        "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip of archive"
      ],
      "metadata": {
        "id": "d6-1LOFkKaFd"
      },
      "id": "d6-1LOFkKaFd"
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "!unzip -nq ./dsl_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNRZRYvbKcya",
        "outputId": "bab2ab88-a5e7-4d2f-b0eb-7d92aad867be"
      },
      "id": "qNRZRYvbKcya",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t __MACOSX\t\t saved_models\n",
            "checkpoints\t models\t\t\t tensorboard_data\n",
            "development.csv  preprocessingGoogle.py  Test_Dataset_Truncated\n",
            "dsl_data.zip\t __pycache__\t\t Train_Dataset_Truncated\n",
            "evaluation.csv\t sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzxQBYDlLl6f",
        "outputId": "88319bc4-b443-4328-f10a-6954666de7cd"
      },
      "id": "lzxQBYDlLl6f",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "audio\t\t __MACOSX\t\t saved_models\n",
            "checkpoints\t models\t\t\t tensorboard_data\n",
            "development.csv  preprocessingGoogle.py  Test_Dataset_Truncated\n",
            "dsl_data.zip\t __pycache__\t\t Train_Dataset_Truncated\n",
            "evaluation.csv\t sample_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for finding optimal length of audio files"
      ],
      "metadata": {
        "id": "nNVJJQ8Qh42Q"
      },
      "id": "nNVJJQ8Qh42Q"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from pydub import AudioSegment\n",
        "\n",
        "folder_path = './audio'\n",
        "\n",
        "def scan_folder(folder):\n",
        "  duration_count = {}\n",
        "  for root, dirs, files in os.walk(folder):\n",
        "    for file in files:\n",
        "      if file.endswith(\".wav\"):\n",
        "        file_path = os.path.join(root, file)\n",
        "        audio = AudioSegment.from_wav(file_path)\n",
        "        duration = len(audio)\n",
        "        if duration in duration_count:\n",
        "          duration_count[duration] += 1\n",
        "        else:\n",
        "            duration_count[duration] = 1\n",
        "  return duration_count\n",
        "\n",
        "def create_dataframe(duration_count):\n",
        "  data = {\"Duration of audio file\": list(duration_count.keys()), \n",
        "            \"Number of audio files with that duration\": list(duration_count.values())}\n",
        "  df = pd.DataFrame(data)\n",
        "  df = df.sort_values(by='Number of audio files with that duration', ascending=False)\n",
        "  return df\n",
        "\n",
        "\n",
        "# find the percentage. The duration returned in second is the size that include 1-percentage inside\n",
        "\n",
        "def find_duration(folder_path, percentage_files=0.9):\n",
        "  duration_count = {}\n",
        "  for root, dirs, files in os.walk(folder_path):\n",
        "    for file in files:\n",
        "      if file.endswith(\".wav\"):\n",
        "        file_path = os.path.join(root, file)\n",
        "        #print(file_path)\n",
        "        audio = AudioSegment.from_wav(file_path)\n",
        "        duration = len(audio) / 1000 #convert from ms to sec\n",
        "        if duration in duration_count:\n",
        "          duration_count[duration] += 1\n",
        "        else:\n",
        "          duration_count[duration] = 1\n",
        "    total_files = sum(duration_count.values())\n",
        "    target_files = total_files * percentage_files\n",
        "    current_count = 0\n",
        "    for duration, count in sorted(duration_count.items()):\n",
        "      current_count += count\n",
        "      if current_count >= target_files:\n",
        "        duration = round(duration)\n",
        "        print(f\"Duration of audio that makes {percentage_files*100}% of the files have that duration is: {duration} seconds\")\n",
        "        return duration\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "duration = find_duration(folder_path)\n",
        "length_calculated = duration"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZsUS3bvh9X9",
        "outputId": "69abaf7f-8d62-4a74-8b7a-90332e0ce683"
      },
      "id": "wZsUS3bvh9X9",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration of audio that makes 90.0% of the files have that duration is: 4 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b824503",
      "metadata": {
        "id": "8b824503"
      },
      "source": [
        " # Preprocessing for Train dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4da2cb4f",
      "metadata": {
        "id": "4da2cb4f"
      },
      "outputs": [],
      "source": [
        "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
        "def process_file(file_path, flag):\n",
        "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
        "    if file_path_exists:\n",
        "        new_sr=16000\n",
        "        # identifier care\n",
        "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
        "        identifier = str(int(identifier))\n",
        "        # label constructor\n",
        "        label = \"\"\n",
        "        if flag == 1:\n",
        "            label  += \"_\"\n",
        "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
        "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
        "            label  += action + object\n",
        "        # If no label available, code will just go on\n",
        "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
        "        y, sr = librosa.load('./'+file_path)\n",
        "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
        "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
        "        y_truncated = y_truncated[:int(pr.length_calculated*new_sr)] #if longer\n",
        "        target_length = pr.length_calculated * new_sr\n",
        "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
        "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "acdacf47",
      "metadata": {
        "scrolled": true,
        "id": "acdacf47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd221728-c1e8-46e8-b347-79bfd7d11e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution ended\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('./development.csv', sep=',')\n",
        "new_folder_path = './Train_Dataset_Truncated/'\n",
        "\n",
        "folder_path = './audio/'\n",
        "\n",
        "if not os.path.isdir(new_folder_path):\n",
        "  os.makedirs(new_folder_path) # hoping to have write permissions set\n",
        "if not os.listdir(new_folder_path):\n",
        "  with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
        "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "      dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
        "      dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCK MICROSOFT AND FUCK THE FUCKING IDEA OF \n",
        "      dirpath = dirpath[dirpath.index(\"/\")+1:] # FUCKING USING \\ FOR PATH!!!!!!!!!!111oneone!!1!!!\n",
        "      dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "      for filename in filenames:\n",
        "        file_path = os.path.join(dirpath, filename)\n",
        "        file_path = file_path.replace(\"\\\\\", \"/\")\n",
        "        executor.submit(process_file, file_path, 1)\n",
        "# print(df)\n",
        "print(\"Execution ended\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2ef26e",
      "metadata": {
        "id": "8f2ef26e"
      },
      "source": [
        " # Preprocessing for Evaluation dataset files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "66b4ba13",
      "metadata": {
        "id": "66b4ba13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01ddcc1-3b7a-445d-b83d-1345084fad88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Execution ended\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('./evaluation.csv', sep=',')\n",
        "new_folder_path = './Test_Dataset_Truncated/'\n",
        "folder_path = './audio/'\n",
        "\n",
        "if not os.path.isdir(new_folder_path):\n",
        "    os.makedirs(new_folder_path)\n",
        "\n",
        "if not os.listdir(new_folder_path):\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
        "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
        "            for filename in filenames:\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
        "                executor.submit(process_file, file_path, 0)\n",
        "\n",
        "print(\"Execution ended\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c41b90",
      "metadata": {
        "id": "69c41b90"
      },
      "source": [
        "# Auto - updating labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "c0749be3",
      "metadata": {
        "id": "c0749be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3b411f-8cf0-488a-dbd5-ed1303f92adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['change languagenone', 'activatemusic', 'deactivatelights', 'increasevolume', 'decreasevolume', 'increaseheat', 'decreaseheat']\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('./development.csv', sep=',')\n",
        "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
        "distinct_values = df['labels'].unique()\n",
        "\n",
        "LABELS = []\n",
        "LABELS = distinct_values.tolist()\n",
        "\n",
        "print(LABELS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frame_length_in_s = 0.032\n",
        "frame_step_in_s  = frame_length_in_s\n",
        "\n",
        "PREPROCESSING_ARGS = {\n",
        "    'downsampling_rate': 16000,\n",
        "    'frame_length_in_s': frame_length_in_s,\n",
        "    'frame_step_in_s': frame_step_in_s,\n",
        "}\n",
        "\n",
        "final_sparsity = 0.01\n",
        "\n",
        "num_mel_bins = (int) ((16000 - 16000 * PREPROCESSING_ARGS['frame_length_in_s'])/(16000*PREPROCESSING_ARGS['frame_step_in_s']))+1\n",
        "#print(num_mel_bins)\n",
        "\n",
        "PREPROCESSING_ARGS = {\n",
        "    **PREPROCESSING_ARGS,\n",
        "    'num_mel_bins': num_mel_bins,\n",
        "    'lower_frequency': 80,\n",
        "    'upper_frequency': 8000,\n",
        "}\n",
        "\n",
        "downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n",
        "sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n",
        "frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n",
        "#print(\"Frame_length: {}\".format(frame_length))\n",
        "frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n",
        "#print(\"Frame_length: {}\".format(frame_step))\n",
        "num_spectrogram_bins = frame_length // 2 + 1\n",
        "num_mel_bins = PREPROCESSING_ARGS['num_mel_bins']\n",
        "lower_frequency = PREPROCESSING_ARGS['lower_frequency']\n",
        "upper_frequency = PREPROCESSING_ARGS['upper_frequency']\n",
        "\n",
        "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
        "    num_mel_bins=num_mel_bins,\n",
        "    num_spectrogram_bins=num_spectrogram_bins,\n",
        "    sample_rate=downsampling_rate,\n",
        "    lower_edge_hertz=lower_frequency,\n",
        "    upper_edge_hertz=upper_frequency\n",
        ")\n",
        "\n",
        "def preprocess(filename):\n",
        "    audio_binary = tf.io.read_file(filename)\n",
        "\n",
        "    path_parts = tf.strings.split(filename, '_')\n",
        "    path_end = path_parts[-1]\n",
        "    file_parts = tf.strings.split(path_end, '.')\n",
        "    true_label = file_parts[0]\n",
        "    label_id = tf.argmax(true_label == LABELS)\n",
        "    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n",
        "    audio = tf.squeeze(audio, axis=-1) #all our audio are mono, drop extra axis\n",
        "    audio_padded = audio\n",
        "    stft = tf.signal.stft(\n",
        "        audio,\n",
        "        frame_length=frame_length,\n",
        "        frame_step=frame_step,\n",
        "        fft_length=frame_length\n",
        "    )\n",
        "    spectrogram = tf.abs(stft)\n",
        "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
        "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
        "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)  # channel axis\n",
        "    mfcss = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
        "\n",
        "    return mfcss, label_id\n"
      ],
      "metadata": {
        "id": "UTKzc5NPgtv4"
      },
      "id": "UTKzc5NPgtv4",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "18c5a54e",
      "metadata": {
        "id": "18c5a54e"
      },
      "source": [
        "# Model creation and fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a5698194",
      "metadata": {
        "id": "a5698194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c1eb78f-0723-4258-f326-b832bcff2273"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, help='Choosing eval_percentage', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "parser = ap.ArgumentParser()\n",
        "\n",
        "parser.add_argument('--batch_size', default=32, type=int, help=\"Choosing batch size default is 32\")\n",
        "parser.add_argument('--initial_learning_rate', default=0.03, type=float, help=\"Choosing initial_learning_rate\")\n",
        "parser.add_argument('--end_learning_rate', default=0.001, type=float, help=\"Choosing end_learning_rate\")\n",
        "parser.add_argument('--epochs', default=50, type=int, help=\"Choosing epochs\")\n",
        "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
        "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
        "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
        "parser.add_argument('--alpha', default=1, type=float, help=\"Choosing alpha\")\n",
        "\n",
        "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")\n",
        "#,'--eval_percentage','0.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5355da28",
      "metadata": {
        "id": "5355da28"
      },
      "source": [
        "Parser arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0cbb23f1",
      "metadata": {
        "id": "0cbb23f1"
      },
      "outputs": [],
      "source": [
        "args = parser.parse_args(['--epochs','100','--alpha','0.125','--batch_size','8','--pruning_initial_step','0.9','--initial_learning_rate','0.03','--end_learning_rate','0.001'])\n",
        "# args = parser.parse_args()\n",
        "num_units = 512"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169533d1",
      "metadata": {
        "id": "169533d1"
      },
      "source": [
        "## This part of the code exist to manage all the folders\n",
        "## Please be careful, if the directories tree is not respected, the code will not work properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3c6a22f4",
      "metadata": {
        "id": "3c6a22f4"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "795af38c",
      "metadata": {
        "id": "795af38c"
      },
      "outputs": [],
      "source": [
        "# Useful to save tensorboard data\n",
        "log_dir_tensorboard = './tensorboard_data/'\n",
        "if not os.path.isdir(log_dir_tensorboard):\n",
        "    os.makedirs(log_dir_tensorboard)\n",
        "#runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
        "#tb_run = max(runs) + 1 if runs else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "6c7ef7f6",
      "metadata": {
        "id": "6c7ef7f6"
      },
      "outputs": [],
      "source": [
        "# Folder creation\n",
        "train_ds_location      = './Train_Dataset_Truncated/'\n",
        "log_dir_model          = './models/'\n",
        "#run_{}_\n",
        "model_name             = 'epochs_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(args.epochs,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha)\n",
        "checkpoint_path        = './checkpoints/' + model_name\n",
        "#check_point_file_name  = checkpoint_path+'.ckpt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a1c4d4b9",
      "metadata": {
        "id": "a1c4d4b9"
      },
      "outputs": [],
      "source": [
        "# If folders to not exist -> create them\n",
        "# This code will not check for the dataset folders, the code above must be executed\n",
        "if not os.path.isdir(log_dir_model):\n",
        "    os.makedirs(log_dir_model)\n",
        "if not os.path.isdir(checkpoint_path):\n",
        "    os.makedirs(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb853db5",
      "metadata": {
        "id": "bb853db5"
      },
      "source": [
        " # Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2bf47be3",
      "metadata": {
        "id": "2bf47be3"
      },
      "outputs": [],
      "source": [
        "file_paths = []\n",
        "\n",
        "for filename in os.listdir(train_ds_location):\n",
        "    file_path = os.path.join(train_ds_location, filename)\n",
        "    file_paths.append(file_path)\n",
        "random.shuffle(file_paths)\n",
        "num_test_files = int(len(file_paths) * args.test_percentage)\n",
        "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
        "#not using eval dataset\n",
        "\n",
        "\n",
        "# num_eval_files = num_eval_files\n",
        "\n",
        "# it is shuffled, so i can do this\n",
        "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
        "#train_paths    = file_paths[num_test_files:]\n",
        "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
        "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_paths))\n",
        "print(len(test_paths))\n",
        "print(len(eval_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn1FluvO5Opv",
        "outputId": "82a8305e-d6e6-409d-b4a8-0d63dd8a0db2"
      },
      "id": "Qn1FluvO5Opv",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6406\n",
            "1970\n",
            "1478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b36277",
      "metadata": {
        "id": "d8b36277"
      },
      "source": [
        "# Preprocessing data and model creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "b8ed3736",
      "metadata": {
        "id": "b8ed3736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72c8e1d-7b0d-402c-f90b-8139f44126c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Shape: (8, 62, 31, 1)\n",
            "Data Shape: (62, 31, 1)\n",
            "Labels: tf.Tensor([3 6 6 5 3 3 3 0], shape=(8,), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
        "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
        "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
        "\n",
        "train_ds       = train_ds.map(preprocess).batch(args.batch_size).cache()\n",
        "val_ds         = val_ds.map(preprocess).batch(args.batch_size)\n",
        "test_ds        = test_ds.map(preprocess).batch(args.batch_size)\n",
        "\n",
        "for example_batch, example_labels in train_ds.take(1):\n",
        "  print('Batch Shape:', example_batch.shape)\n",
        "  print('Data Shape:', example_batch.shape[1:])\n",
        "  print('Labels:', example_labels)\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
        "end_step            = int(len(train_ds) * args.epochs)\n",
        "pruning_params      = {\n",
        "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
        "        initial_sparsity=args.initial_sparsity,\n",
        "        final_sparsity=final_sparsity,\n",
        "        begin_step=begin_step,\n",
        "        end_step=end_step\n",
        "    )\n",
        "}\n",
        "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
        "\n",
        "# model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)\n",
        "# model_name += '.h5'\n",
        "\n",
        "hparams = {\n",
        "'num_units' : num_units,\n",
        "'alpha_rate': args.alpha,\n",
        "'epochs'    : args.epochs,\n",
        "'batch_size': args.batch_size,\n",
        "}\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
        "        use_bias=False, padding='valid'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
        "            use_bias=False, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
        "        use_bias=False, padding='same'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.ReLU(),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(units=len(LABELS)),\n",
        "    tf.keras.layers.Softmax()\n",
        "    ])\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "eb8ec704",
      "metadata": {
        "id": "eb8ec704",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f78673e-68fd-4826-ff1a-959e9ca64bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(62, 31, 1)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_conv2d   (None, 30, 15, 64)       1154      \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_batch_n  (None, 30, 15, 64)       257       \n",
            " ormalization (PruneLowMagni                                     \n",
            " tude)                                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_re_lu (  (None, 30, 15, 64)       1         \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d_  (None, 30, 15, 64)       73730     \n",
            " 1 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_batch_n  (None, 30, 15, 64)       257       \n",
            " ormalization_1 (PruneLowMag                                     \n",
            " nitude)                                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_re_lu_1  (None, 30, 15, 64)       1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d_  (None, 30, 15, 64)       73730     \n",
            " 2 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_batch_n  (None, 30, 15, 64)       257       \n",
            " ormalization_2 (PruneLowMag                                     \n",
            " nitude)                                                         \n",
            "                                                                 \n",
            " prune_low_magnitude_re_lu_2  (None, 30, 15, 64)       1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_global_  (None, 64)               1         \n",
            " average_pooling2d (PruneLow                                     \n",
            " Magnitude)                                                      \n",
            "                                                                 \n",
            " prune_low_magnitude_dense (  (None, 7)                905       \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_softmax  (None, 7)                1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 150,295\n",
            "Trainable params: 75,143\n",
            "Non-trainable params: 75,152\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
        "print(example_batch.shape[1:])\n",
        "model_for_pruning.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model fitting"
      ],
      "metadata": {
        "id": "dSA36b8J31Um"
      },
      "id": "dSA36b8J31Um"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b35a1c",
      "metadata": {
        "scrolled": true,
        "id": "69b35a1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22832a01-9985-44e7-8599-9c738d15a14f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No previous check_point found.\n",
            "Epoch 1/100\n",
            " 12/801 [..............................] - ETA: 13:29 - loss: 2.2830 - sparse_categorical_accuracy: 0.2396"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
        "    initial_learning_rate=args.initial_learning_rate,\n",
        "    end_learning_rate=args.end_learning_rate,\n",
        "    decay_steps=len(train_ds) * args.epochs,\n",
        ")\n",
        "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
        "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
        "\n",
        "callbacks = [ tf.keras.callbacks.ModelCheckpoint(filepath=log_dir_tensorboard+model_name,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1),\n",
        "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
        "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+model_name, histogram_freq=1) , hp.KerasCallback(log_dir_tensorboard+model_name, hparams),\n",
        "             ]\n",
        "\n",
        "\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "if os.path.exists(log_dir_tensorboard+model_name+'.ckpt'):\n",
        "    print(\"Checkpoint found, loading...\")\n",
        "    model.load_weights(log_dir_tensorboard+model_name+'.ckpt')\n",
        "    with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"r\") as file:\n",
        "        contents = file.read()\n",
        "        previous_epoch_run = int(contents)\n",
        "        previous_epoch_run = previous_epoch_run\n",
        "    print(\"Restoring from epoch : {}\".format(previous_epoch_run))\n",
        "else:\n",
        "    print(\"No previous check_point found.\")\n",
        "    previous_epoch_run = 0\n",
        "\n",
        "#validation data is test_ds validation_data=val_ds,\n",
        "    \n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=args.epochs, callbacks=callbacks,verbose=1,initial_epoch=previous_epoch_run) #it was valds\n",
        "\n",
        "with open(log_dir_tensorboard+model_name+\"epochs.txt\", \"w\") as file:\n",
        "    file.write(str(args.epochs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab63e37",
      "metadata": {
        "id": "eab63e37"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "\n",
        "training_loss = history.history['loss'][-1]\n",
        "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
        "val_loss = history.history['val_loss'][-1]\n",
        "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
        "\n",
        "print(f'Training Loss: {training_loss:.4f}')\n",
        "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
        "print()\n",
        "print(f'Validation Loss: {val_loss:.4f}')\n",
        "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
        "print()\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63692d0",
      "metadata": {
        "id": "a63692d0"
      },
      "outputs": [],
      "source": [
        "with open(log_dir_model+model_name+\".txt\", \"w\") as file:\n",
        "    file.write(model_name)\n",
        "    file.write(\"\\n\")\n",
        "    file.write(\"Execution lasted: \" + str(args.epochs))\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nTraining Loss: {training_loss:.4f}')\n",
        "    file.write(f'\\nTraining Accuracy: {training_accuracy*100.:.2f}%')\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nValidation Loss: {val_loss:.4f}')\n",
        "    file.write(f'\\nValidation Accuracy: {val_accuracy*100.:.2f}%')\n",
        "    file.write(\"\\n\")\n",
        "    file.write(f'\\nTest Loss: {test_loss:.4f}')\n",
        "    file.write(f'\\nTest Accuracy: {test_accuracy*100.:.2f}%')\n",
        "    \n",
        "saved_model_dir = f'./saved_models/last_model_used'\n",
        "if not os.path.exists(saved_model_dir):\n",
        "    os.makedirs(saved_model_dir)\n",
        "model.save(saved_model_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(log_dir_model+model_name)\n",
        "with open(log_dir_model+model_name+\".txt\", \"r\") as file:\n",
        "        contents = file.read()\n",
        "        print(contents)\n",
        "    "
      ],
      "metadata": {
        "id": "Ij00I6v5eHGV"
      },
      "id": "Ij00I6v5eHGV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}