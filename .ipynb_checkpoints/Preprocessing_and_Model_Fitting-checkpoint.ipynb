{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f9fa83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow_io as tfio\n",
    "import preprocessing as pr\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678282d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Be sure to have tensorboard, this code will be commented in final release\"\n"
     ]
    }
   ],
   "source": [
    "!echo \"Be sure to have tensorboard, this code will be commented in final release\"\n",
    "#!tensorboard --logdir ../../datasets/dsl_data/tensorboard_data/ &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba00ab3c",
   "metadata": {},
   "source": [
    " # Preprocessing for Train dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b3d2cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_file better to be implemented here with a boolean value that checks if i am processing train_dataset or eval file\n",
    "def process_file(file_path, flag):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    if file_path_exists:\n",
    "        new_sr=16000\n",
    "        # identifier care\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # label constructor\n",
    "        label = \"\"\n",
    "        if flag == 1:\n",
    "            label  += \"_\"\n",
    "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "            label  += action + object\n",
    "        # If no label available, code will just go on\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
    "        y, sr = librosa.load('../../datasets/'+file_path)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "        y_truncated = y_truncated[:int(4*new_sr)] #if longer\n",
    "        target_length = 4 * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab73efec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path) # hoping to have write permissions set\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 1)\n",
    "# print(df)\n",
    "print(\"Execution ended\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f571d0b9",
   "metadata": {},
   "source": [
    " # Preprocessing for Evaluation dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91b3ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution ended\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/evaluation.csv', sep=',')\n",
    "new_folder_path = '../../datasets/dsl_data/Test_Dataset_Truncated/'\n",
    "folder_path = '../../datasets/dsl_data/'\n",
    "\n",
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "\n",
    "if not os.listdir(new_folder_path):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 0)\n",
    "\n",
    "print(\"Execution ended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164548f5",
   "metadata": {},
   "source": [
    "# Auto - updating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf24164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../datasets/dsl_data/development.csv', sep=',')\n",
    "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
    "distinct_values = df['labels'].unique()\n",
    "\n",
    "result = 'LABELS = ['\n",
    "for value in distinct_values:\n",
    "    result += \"'\" + str(value) + \"', \"\n",
    "\n",
    "result = result[:-2] + ']\\n' # lazy workaround, the last label has a comma that is bad.. this is also bad.\n",
    "\n",
    "with open(\"preprocessing.py\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Update the 4th line\n",
    "lines[3] = result\n",
    "lines[4] = \"# This is the file genarated that has the Labels that i must use for training\\n\"\n",
    "\n",
    "with open(\"preprocessing.py\", \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204a20ce",
   "metadata": {},
   "source": [
    "# Model creation and fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e98388d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--eval_percentage'], dest='eval_percentage', nargs=None, const=None, default=0.15, type=<class 'float'>, choices=None, required=False, help='Choosing eval_percentage', metavar=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ap.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', default=pr.TRAINING_ARGS['batch_size'], type=int, help=\"Choosing batch size default is 32\")\n",
    "parser.add_argument('--initial_learning_rate', default=pr.TRAINING_ARGS['initial_learning_rate'], type=float, help=\"Choosing initial_learning_rate\")\n",
    "parser.add_argument('--end_learning_rate', default=pr.TRAINING_ARGS['end_learning_rate'], type=float, help=\"Choosing end_learning_rate\")\n",
    "parser.add_argument('--epochs', default=pr.TRAINING_ARGS['epochs'], type=int, help=\"Choosing epochs\")\n",
    "parser.add_argument('--test_percentage', default=0.2, type=float, help=\"Choosing test_percentage\")\n",
    "parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
    "parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
    "parser.add_argument('--alpha', default=pr.alpha, type=float, help=\"Choosing alpha\")\n",
    "\n",
    "parser.add_argument('--eval_percentage', default=0.15, type=float, help=\"Choosing eval_percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338b1b1",
   "metadata": {},
   "source": [
    "Parser arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9516711",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(['--epochs','10','--batch_size','32','--initial_learning_rate','0.03','--end_learning_rate','0.03'])\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1edbce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "train_ds_location   = '../../datasets/dsl_data/Train_Dataset_Truncated/'\n",
    "log_dir_tensorboard = '../../datasets/dsl_data/tensorboard_data/'\n",
    "log_dir_model       = '../../datasets/dsl_data/models/'\n",
    "\n",
    "runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
    "tb_run = max(runs) + 1 if runs else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64033086",
   "metadata": {},
   "source": [
    "Obtaining Test data from train data, using shuffle and avoiding retaking same data on different runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b16dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for filename in os.listdir(train_ds_location):\n",
    "    file_path = os.path.join(train_ds_location, filename)\n",
    "    file_paths.append(file_path)\n",
    "random.shuffle(file_paths)\n",
    "num_test_files = int(len(file_paths) * args.test_percentage)\n",
    "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
    "\n",
    "# it is shuffled, so i can do this\n",
    "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
    "train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
    "eval_paths     = file_paths[-num_eval_files:]                # until the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39572e",
   "metadata": {},
   "source": [
    "Preprocessing data and model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4778b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape: (32, 125, 31, 1)\n",
      "Data Shape: (125, 31, 1)\n",
      "Labels: tf.Tensor([5 1 4 3 4 3 4 4 3 1 5 3 2 6 2 3 4 5 0 4 1 3 4 3 3 3 3 6 4 6 1 1], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
    "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
    "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
    "\n",
    "train_ds       = train_ds.map(pr.preprocess).batch(args.batch_size).cache()\n",
    "val_ds         = val_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "test_ds        = test_ds.map(pr.preprocess).batch(args.batch_size)\n",
    "\n",
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Batch Shape:', example_batch.shape)\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "begin_step          = int(len(train_ds) * args.epochs * args.pruning_initial_step)\n",
    "end_step            = int(len(train_ds) * args.epochs)\n",
    "pruning_params      = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=args.initial_sparsity,\n",
    "        final_sparsity=pr.final_sparsity,\n",
    "        begin_step=begin_step,\n",
    "        end_step=end_step\n",
    "    )\n",
    "}\n",
    "custom_objects      = {'PruneLowMagnitude': prune_low_magnitude}\n",
    "\n",
    "model_name          = 'model_'+str(args.batch_size)+'_'+str(args.alpha)+'.h5'\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
    "        use_bias=False, padding='valid'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "            use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.Conv2D(filters=int(128 * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "        use_bias=False, padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(pr.LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "062b98cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d   (None, 62, 15, 128)      2306      \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization (PruneLowMagni                                     \n",
      " tude)                                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu (  (None, 62, 15, 128)      1         \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_1 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_1  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 62, 15, 128)      294914    \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_batch_n  (None, 62, 15, 128)      513       \n",
      " ormalization_2 (PruneLowMag                                     \n",
      " nitude)                                                         \n",
      "                                                                 \n",
      " prune_low_magnitude_re_lu_2  (None, 62, 15, 128)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_global_  (None, 128)              1         \n",
      " average_pooling2d (PruneLow                                     \n",
      " Magnitude)                                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 8)                2058      \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_softmax  (None, 8)                1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 595,736\n",
      "Trainable params: 297,864\n",
      "Non-trainable params: 297,872\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# this model uses Transfer Learning... I mean, we transferred a model developed for another course to this course\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599bc6e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "201/201 [==============================] - 74s 350ms/step - loss: 1.8369 - sparse_categorical_accuracy: 0.2562 - val_loss: 4.6457 - val_sparse_categorical_accuracy: 0.2530\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 66s 330ms/step - loss: 1.7825 - sparse_categorical_accuracy: 0.2672 - val_loss: 3.6145 - val_sparse_categorical_accuracy: 0.1116\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 60s 300ms/step - loss: 1.7577 - sparse_categorical_accuracy: 0.2743 - val_loss: 2.4957 - val_sparse_categorical_accuracy: 0.1150\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 67s 332ms/step - loss: 1.7235 - sparse_categorical_accuracy: 0.2874 - val_loss: 2.1248 - val_sparse_categorical_accuracy: 0.1705\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 63s 315ms/step - loss: 1.6798 - sparse_categorical_accuracy: 0.3008 - val_loss: 1.8917 - val_sparse_categorical_accuracy: 0.1096\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 58s 290ms/step - loss: 1.5852 - sparse_categorical_accuracy: 0.3501 - val_loss: 1.9131 - val_sparse_categorical_accuracy: 0.1333\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 58s 288ms/step - loss: 1.4908 - sparse_categorical_accuracy: 0.3953 - val_loss: 1.9597 - val_sparse_categorical_accuracy: 0.2233\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 58s 289ms/step - loss: 1.4120 - sparse_categorical_accuracy: 0.4308 - val_loss: 2.0029 - val_sparse_categorical_accuracy: 0.2625\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 58s 288ms/step - loss: 1.3467 - sparse_categorical_accuracy: 0.4622 - val_loss: 1.4381 - val_sparse_categorical_accuracy: 0.4465\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 58s 288ms/step - loss: 1.2839 - sparse_categorical_accuracy: 0.4931 - val_loss: 1.6463 - val_sparse_categorical_accuracy: 0.3931\n",
      "62/62 [==============================] - 6s 94ms/step - loss: 1.6268 - sparse_categorical_accuracy: 0.4015\n",
      "Training Loss: 1.2839\n",
      "Training Accuracy: 49.31%\n",
      "\n",
      "Validation Loss: 1.6463\n",
      "Validation Accuracy: 39.31%\n",
      "\n",
      "Test Loss: 1.6268\n",
      "Test Accuracy: 40.15%\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=args.initial_learning_rate,\n",
    "    end_learning_rate=args.end_learning_rate,\n",
    "    decay_steps=len(train_ds) * args.epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "callbacks = [ tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=log_dir_model+model_name, \n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    save_freq='epoch'),\n",
    "             tfmot.sparsity.keras.UpdatePruningStep(), \n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+'run_{}_epochs_{}_batch_size_{}_pruning_initial_step_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_pruning_initial_step_{}_initial_sparsity_{}_alpha_{}'.format(tb_run,args.epochs,args.batch_size,args.pruning_initial_step,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.pruning_initial_step,args.initial_sparsity,args.alpha), histogram_freq=1)]\n",
    "\n",
    "\n",
    "model_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "history = model_for_pruning.fit(train_ds, epochs=args.epochs, validation_data=val_ds,callbacks=callbacks) #it was valds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2263e0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 5s 71ms/step - loss: 1.6268 - sparse_categorical_accuracy: 0.4015\n",
      "Training Loss: 1.2839\n",
      "Training Accuracy: 49.31%\n",
      "\n",
      "Validation Loss: 1.6463\n",
      "Validation Accuracy: 39.31%\n",
      "\n",
      "Test Loss: 1.6268\n",
      "Test Accuracy: 40.15%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f9eff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
