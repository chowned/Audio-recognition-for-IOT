{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow import keras\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import argparse as ap\n",
    "from pydub import AudioSegment\n",
    "import sys\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463abacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sr=8000\n",
    "LABELS = []\n",
    "num_units = 512\n",
    "os.chdir('./datasets/dsl_data/')\n",
    "folder_path = './audio'\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1332c",
   "metadata": {},
   "source": [
    "The code was implemented to run on a headless server, then it was converted to a notebook script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426bed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ap.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--batch_size', default=32, type=int, help=\"Choosing batch size default is 32\")\n",
    "parser.add_argument('--initial_learning_rate', default=0.01, type=float, help=\"Choosing initial_learning_rate\")\n",
    "parser.add_argument('--end_learning_rate', default=0.005, type=float, help=\"Choosing end_learning_rate\")\n",
    "parser.add_argument('--epochs', default=200, type=int, help=\"Choosing epochs\")\n",
    "parser.add_argument('--test_percentage', default=0.20, type=float, help=\"Choosing test_percentage\")\n",
    "# parser.add_argument('--pruning_initial_step', default=0.2, type=float, help=\"Choosing pruning_initial_step\")\n",
    "# parser.add_argument('--initial_sparsity', default=0.40, type=float, help=\"Choosing initial_sparsity\")\n",
    "parser.add_argument('--alpha', default=1, type=float, help=\"Choosing alpha\")\n",
    "parser.add_argument('--eval_percentage', default=0.0, type=float, help=\"Choosing eval_percentage\")\n",
    "\n",
    "args = parser.parse_args(['--alpha','1.0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66452985",
   "metadata": {},
   "source": [
    "HP for the audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3d7676",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_length_in_s = 0.04#0.032*2 # /2 for resnet18\n",
    "frame_step_in_s  = frame_length_in_s#frame_length_in_s\n",
    "\n",
    "PREPROCESSING_ARGS = {\n",
    "    'downsampling_rate': new_sr,\n",
    "    'frame_length_in_s': frame_length_in_s,\n",
    "    'frame_step_in_s': frame_step_in_s,\n",
    "}\n",
    "\n",
    "num_mel_bins = (int) ((new_sr - new_sr * PREPROCESSING_ARGS['frame_length_in_s'])/(new_sr*PREPROCESSING_ARGS['frame_step_in_s']))+1\n",
    "# print(num_mel_bins)\n",
    "\n",
    "PREPROCESSING_ARGS = {\n",
    "    **PREPROCESSING_ARGS,\n",
    "    'num_mel_bins': num_mel_bins,\n",
    "    'lower_frequency': 20,   #40\n",
    "    'upper_frequency': new_sr/2, #4000\n",
    "}\n",
    "\n",
    "downsampling_rate = PREPROCESSING_ARGS['downsampling_rate']\n",
    "sampling_rate_int64 = tf.cast(downsampling_rate, tf.int64)\n",
    "frame_length = int(downsampling_rate * PREPROCESSING_ARGS['frame_length_in_s'])\n",
    "#print(\"Frame_length: {}\".format(frame_length))\n",
    "frame_step = int(downsampling_rate * PREPROCESSING_ARGS['frame_step_in_s'])\n",
    "#print(\"Frame_length: {}\".format(frame_step))\n",
    "num_spectrogram_bins = frame_length // 2 + 1\n",
    "num_mel_bins = PREPROCESSING_ARGS['num_mel_bins']\n",
    "lower_frequency = PREPROCESSING_ARGS['lower_frequency']\n",
    "upper_frequency = PREPROCESSING_ARGS['upper_frequency']\n",
    "\n",
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "    num_mel_bins=num_mel_bins,\n",
    "    num_spectrogram_bins=num_spectrogram_bins,\n",
    "    sample_rate=downsampling_rate,\n",
    "    lower_edge_hertz=lower_frequency,\n",
    "    upper_edge_hertz=upper_frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de282eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(filename):\n",
    "    audio_binary = tf.io.read_file(filename)\n",
    "\n",
    "    path_parts = tf.strings.split(filename, '_')\n",
    "    path_end = path_parts[-1]\n",
    "    file_parts = tf.strings.split(path_end, '.')\n",
    "    true_label = file_parts[0]\n",
    "    label_id = tf.argmax(true_label == LABELS)\n",
    "    audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n",
    "    audio = tf.squeeze(audio, axis=-1) #all our audio are mono, drop extra axis\n",
    "    stft = tf.signal.stft(\n",
    "        audio,\n",
    "        frame_length=frame_length,\n",
    "        frame_step=frame_step,\n",
    "        fft_length=frame_length\n",
    "    )\n",
    "    spectrogram = tf.abs(stft)\n",
    "    mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "    log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)  # channel axis\n",
    "    mfcss = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "\n",
    "    return mfcss, label_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13efcb",
   "metadata": {},
   "source": [
    "Classes for Early Stopping, thanks to Prof Pagliari for the support, 10 points to Gryffindor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2859f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyThresholdCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(MyThresholdCallback, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        val_acc = logs[\"val_sparse_categorical_accuracy\"]\n",
    "        if val_acc >= self.threshold:\n",
    "            self.model.stop_training = True\n",
    "\n",
    "class MyThresholdCallbackTrain(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, threshold):\n",
    "        super(MyThresholdCallbackTrain, self).__init__()\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None): \n",
    "        val_acc = logs[\"sparse_categorical_accuracy\"]\n",
    "        if val_acc >= self.threshold:\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976a3bf",
   "metadata": {},
   "source": [
    "The dataset to be used needs to be processed on its own before actually working. This part is intended for the DSL exam. The audio passed on the network are of random length, this functions are meant to extract the optimal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89defc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_folder(folder): # find audio length\n",
    "  duration_count = {}\n",
    "  for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "      if file.endswith(\".wav\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        audio = AudioSegment.from_wav(file_path)\n",
    "        duration = len(audio)\n",
    "        if duration in duration_count:\n",
    "          duration_count[duration] += 1\n",
    "        else:\n",
    "            duration_count[duration] = 1\n",
    "  return duration_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256005c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(duration_count):\n",
    "  data = {\"Duration of audio file\": list(duration_count.keys()), \n",
    "            \"Number of audio files with that duration\": list(duration_count.values())}\n",
    "  df = pd.DataFrame(data)\n",
    "  df = df.sort_values(by='Number of audio files with that duration', ascending=False)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dc5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duration(folder_path, percentage_files=0.90):\n",
    "  duration_count = {}\n",
    "  for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "      if file.endswith(\".wav\"):\n",
    "        file_path = os.path.join(root, file)\n",
    "        #print(file_path)\n",
    "        audio = AudioSegment.from_wav(file_path)\n",
    "        duration = len(audio) / 1000 #convert from ms to sec\n",
    "        if duration in duration_count:\n",
    "          duration_count[duration] += 1\n",
    "        else:\n",
    "          duration_count[duration] = 1\n",
    "    total_files = sum(duration_count.values())\n",
    "    target_files = total_files * percentage_files\n",
    "    current_count = 0\n",
    "    for duration, count in sorted(duration_count.items()):\n",
    "      current_count += count\n",
    "      if current_count >= target_files:\n",
    "        duration = round(duration)\n",
    "        print(f\"Duration of audio that makes {percentage_files*100}% of the files have that duration is: {duration} seconds\")\n",
    "        return duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9347ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path, flag):\n",
    "    file_path_exists = df[df[\"path\"] == file_path].shape[0] > 0 #flag\n",
    "    if file_path_exists:\n",
    "        # identifier care\n",
    "        identifier = df.loc[df[\"path\"] == file_path, \"Id\"].values[0]\n",
    "        identifier = str(int(identifier))\n",
    "        # label constructor\n",
    "        label = \"\"\n",
    "        if flag == 1: # it means i am using development.csv\n",
    "            label  += \"_\"\n",
    "            action  = df.loc[df[\"path\"] == file_path, \"action\"].values[0]\n",
    "            object  = df.loc[df[\"path\"] == file_path, \"object\"].values[0]\n",
    "            label  += action + object\n",
    "        # If no label available, code will just go on\n",
    "        new_file_path = os.path.join(new_folder_path, identifier + label + '.wav')\n",
    "        #print(new_file_path)\n",
    "        y, sr = librosa.load('../'+file_path)\n",
    "        #print('../'+file_path)\n",
    "        y_truncated = librosa.effects.trim(y, top_db=50, frame_length=2048, hop_length=512, ref=np.max)[0]\n",
    "        y_truncated = librosa.resample(y_truncated, orig_sr=sr, target_sr=new_sr)\n",
    "        y_truncated = y_truncated[:int(length_calculated*new_sr)] #if longer\n",
    "        target_length = length_calculated * new_sr\n",
    "        y_truncated = librosa.util.fix_length(data=y_truncated, size=target_length) # padding, if shorter\n",
    "        sf.write(new_file_path, y_truncated, new_sr, 'PCM_16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_calculated = find_duration(folder_path)\n",
    "df = pd.read_csv('./development.csv', sep=',')\n",
    "new_folder_path = './Train_Dataset_Truncated/'\n",
    "folder_path = '../dsl_data/audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd879969",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(new_folder_path):\n",
    "  os.makedirs(new_folder_path) # hoping to have write permissions set\n",
    "if not os.listdir(new_folder_path):\n",
    "  print(\"Creating dataset files\")\n",
    "  with ThreadPoolExecutor() as executor: # who is your single threaddy?\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "      dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "      dirpath = dirpath[dirpath.index(\"/\")+1:] \n",
    "      for filename in filenames:\n",
    "        file_path = os.path.join(dirpath, filename)\n",
    "        file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "        executor.submit(process_file, file_path, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb85929",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./evaluation.csv', sep=',')\n",
    "new_folder_path = './Test_Dataset_Truncated/'\n",
    "folder_path = '../dsl_data/audio/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a597a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(new_folder_path):\n",
    "    os.makedirs(new_folder_path)\n",
    "if not os.listdir(new_folder_path):\n",
    "    print(\"Creating evaluation files\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            dirpath = dirpath.replace(\"\\\\\", \"/\")\n",
    "            dirpath = dirpath[dirpath.index(\"/\")+1:]\n",
    "            for filename in filenames:\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                file_path = file_path.replace(\"\\\\\", \"/\")\n",
    "                executor.submit(process_file, file_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d95fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./development.csv', sep=',')\n",
    "df['labels'] = df['action'].astype(str) + df['object'].astype(str)\n",
    "distinct_values = df['labels'].unique()\n",
    "\n",
    "LABELS = distinct_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced5ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_tensorboard = './tensorboard_data/'\n",
    "if not os.path.isdir(log_dir_tensorboard):\n",
    "    os.makedirs(log_dir_tensorboard)\n",
    "runs = [int(d.split('_')[1]) for d in os.listdir(log_dir_tensorboard) if 'run_' in d]\n",
    "tb_run = max(runs) + 1 if runs else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_location      = './Train_Dataset_Truncated/'\n",
    "log_dir_model          = './models/'\n",
    "model_name             = 'tb_run_{}_frame_l_{}_epochs_{}_batch_size_{}_initial_learning_rate_{}_end_learning_rate_{}_test_percentage_{}_alpha_{}'.format(tb_run,frame_length_in_s,args.epochs,args.batch_size,args.initial_learning_rate,args.end_learning_rate,args.test_percentage,args.alpha)\n",
    "checkpoint_path        = './checkpoints/' + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821381ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(log_dir_model):\n",
    "    os.makedirs(log_dir_model)\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = []\n",
    "\n",
    "for filename in os.listdir(train_ds_location):\n",
    "    file_path = os.path.join(train_ds_location, filename)\n",
    "    file_paths.append(file_path)\n",
    "random.shuffle(file_paths)\n",
    "num_test_files = int(len(file_paths) * args.test_percentage)\n",
    "num_eval_files = int(len(file_paths) * args.eval_percentage)\n",
    "#not using eval dataset\n",
    "\n",
    "# it is shuffled, so i can do this\n",
    "test_paths     = file_paths[:num_test_files]                 # from 0 to num_test_files\n",
    "train_paths    = file_paths[num_test_files:]\n",
    "#train_paths    = file_paths[num_test_files:-num_eval_files]  # from num_test_files to end-num_eval_files\n",
    "eval_paths     = file_paths[-num_eval_files:]                # until the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f50254",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds       = tf.data.Dataset.list_files(train_paths)\n",
    "val_ds         = tf.data.Dataset.list_files(eval_paths)\n",
    "test_ds        = tf.data.Dataset.list_files(test_paths)\n",
    "\n",
    "train_ds       = train_ds.map(preprocess).batch(args.batch_size).cache()\n",
    "val_ds         = val_ds.map(preprocess).batch(args.batch_size)\n",
    "test_ds        = test_ds.map(preprocess).batch(args.batch_size)\n",
    "\n",
    "for example_batch, example_labels in train_ds.take(1):\n",
    "  print('Data Shape:', example_batch.shape[1:])\n",
    "  print('Labels:', example_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "'num_units' : num_units,\n",
    "'alpha_rate': args.alpha,\n",
    "'frame l'   : frame_length_in_s,\n",
    "'epochs'    : args.epochs,\n",
    "'batch_size': args.batch_size,\n",
    "}\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n",
    "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[2, 2],\n",
    "        use_bias=False, padding='valid', kernel_initializer='glorot_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n",
    "        use_bias=False, padding='same', kernel_initializer='glorot_normal'),\n",
    "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "            use_bias=False, padding='same', kernel_initializer='glorot_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.DepthwiseConv2D(kernel_size=[3, 3], strides=[1, 1], \n",
    "        use_bias=False, padding='same', kernel_initializer='glorot_normal'),\n",
    "    tf.keras.layers.Conv2D(filters=int(num_units * args.alpha), kernel_size=[3, 3], strides=[1, 1],\n",
    "        use_bias=False, padding='same', kernel_initializer='glorot_normal'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.ReLU(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(units=len(LABELS)),\n",
    "    tf.keras.layers.Softmax()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cb998",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=args.initial_learning_rate,\n",
    "    end_learning_rate=args.end_learning_rate,\n",
    "    decay_steps=len(train_ds) * args.epochs,\n",
    ")\n",
    "optimizer = tf.optimizers.Adam(learning_rate=linear_decay)\n",
    "metrics = [tf.metrics.SparseCategoricalAccuracy()]\n",
    "tensorboard_model_saved = f\"run_{tb_run}\"\n",
    "\n",
    "\n",
    "my_callback_val   = MyThresholdCallback(threshold=0.95)\n",
    "my_callback_train = MyThresholdCallbackTrain(threshold=0.999)\n",
    "\n",
    "callbacks = [ tf.keras.callbacks.ModelCheckpoint(filepath=log_dir_model+model_name+'.ckpt',save_weights_only=True,verbose=1),\n",
    "            #  tfmot.sparsity.keras.UpdatePruningStep(), \n",
    "             keras.callbacks.TensorBoard(log_dir=log_dir_tensorboard+tensorboard_model_saved, histogram_freq=1) , \n",
    "             hp.KerasCallback(log_dir_tensorboard+tensorboard_model_saved, hparams),# val_accuracy\n",
    "             #tf.keras.callbacks.EarlyStopping(monitor='sparse_categorical_accuracy', mode='max', patience=10, min_delta=2.0, restore_best_weights=True, verbose=1, baseline=0.985),\n",
    "            #  my_callback_val, \n",
    "             my_callback_train,\n",
    "             ]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "if os.path.exists(log_dir_model+model_name+'.ckpt'):\n",
    "    print(\"Checkpoint found, loading...\")\n",
    "    model.load_weights(log_dir_model+model_name+'.ckpt')\n",
    "    # with open(log_dir_model+model_name+\"epochs.txt\", \"r\") as file:\n",
    "    #     contents = file.read()\n",
    "    #     previous_epoch_run = int(contents)\n",
    "    #     previous_epoch_run = previous_epoch_run\n",
    "    # print(\"Restoring from epoch : {}\".format(previous_epoch_run))\n",
    "else:\n",
    "    print(\"No previous check_point found.\")\n",
    "    previous_epoch_run = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a6db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, validation_data=test_ds, epochs=args.epochs, callbacks=callbacks,verbose=1,initial_epoch=previous_epoch_run) #it was valds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "\n",
    "training_loss = history.history['loss'][-1]\n",
    "training_accuracy = history.history['sparse_categorical_accuracy'][-1]\n",
    "val_loss = history.history['val_loss'][-1]\n",
    "val_accuracy = history.history['val_sparse_categorical_accuracy'][-1]\n",
    "\n",
    "print(f'Training Loss: {training_loss:.4f}')\n",
    "print(f'Training Accuracy: {training_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Validation Loss: {val_loss:.4f}')\n",
    "print(f'Validation Accuracy: {val_accuracy*100.:.2f}%')\n",
    "print()\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy*100.:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fadf039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "filenames = glob('./Test_Dataset_Truncated/*')\n",
    "\n",
    "with open(f\"Evaluation_Dataset_Result_{tb_run}.csv\", \"w\") as file:\n",
    "    file.write(\"Id,Predicted\")\n",
    "    #file.write(\"\\n\") \n",
    "    file.write(\"\")\n",
    "    for filename in filenames:\n",
    "        identifier = filename.replace(\"\\\\\", \"/\").split('/')[-1].split('.')[0]\n",
    "        #filename = filename.split('/')[-1].split('.')[0]\n",
    "        #print(identifier)\n",
    "        audio_binary = tf.io.read_file(filename)\n",
    "        audio, sampling_rate = tf.audio.decode_wav(audio_binary)\n",
    "        audio = tf.squeeze(audio, axis=-1) #all our audio are mono, drop extra axis\n",
    "        # audio_padded = audio\n",
    "        stft = tf.signal.stft(\n",
    "            audio,\n",
    "            frame_length=frame_length,\n",
    "            frame_step=frame_step,\n",
    "            fft_length=frame_length\n",
    "        )\n",
    "        spectrogram = tf.abs(stft)\n",
    "        mel_spectrogram = tf.matmul(spectrogram, linear_to_mel_weight_matrix)\n",
    "        log_mel_spectrogram = tf.math.log(mel_spectrogram + 1.e-6)\n",
    "        log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)  # channel axis\n",
    "        mfcss = tf.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrogram)\n",
    "        \n",
    "        mfcss = tf.expand_dims(mfcss, 0)\n",
    "        prediction = model.predict(mfcss)\n",
    "        \n",
    "        prediction = np.argmax(prediction[0])\n",
    "        prediction = LABELS[prediction]\n",
    "        \n",
    "        #print(prediction)\n",
    "        file.write(\"\\n{},{}\".format(identifier,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb567d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "timestamp = int(time())\n",
    "modelName = f'model_{tb_run}'\n",
    "\n",
    "saved_model_dir = f'./saved_models/{modelName}'\n",
    "if not os.path.exists(saved_model_dir):\n",
    "    os.makedirs(saved_model_dir)\n",
    "model.save(saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe25537",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{modelName}')\n",
    "tflite_model = converter.convert()\n",
    "tflite_models_dir = './tflite_models'\n",
    "if not os.path.exists(tflite_models_dir):\n",
    "    os.makedirs(tflite_models_dir)\n",
    "tflite_model_name = os.path.join(tflite_models_dir, f'{modelName}.tflite')\n",
    "# tflite_model_name\n",
    "with open(tflite_model_name, 'wb') as fp:\n",
    "    fp.write(tflite_model)\n",
    "\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(tflite_model_name)\n",
    "\n",
    "pruned_tflite_size = os.path.getsize(tflite_model_name) / 1024\n",
    "pruned_zip_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024\n",
    "\n",
    "print(f'Original TFLite Size (pruned model): {pruned_tflite_size:.2f} KB')\n",
    "print(f'ZIP TFLite Size (pruned model): {pruned_zip_size:.2f} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31799e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4a8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kernel IoT",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
